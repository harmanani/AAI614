{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI614: Data Science & its Applications\n",
    "\n",
    "*Notebook 8.5: NLP Analysis of Large Text Datasets*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI614/blob/main/Week%206/Notebook8.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `newsgroups` object contains messages from 20 different message-board categories. We can view the category names by printing `newsgroups.target_names`.\n",
    "\n",
    "**Listing 15. 2. Printing the names of all 20 newsgroups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual text information is stored within the `newsgroups.data` attribute. The attribute contains a list of posted texts. Thus, `newsgroups.data[0]` contains the text of the first stored newsgroup post.\n",
    "\n",
    "**Print the first newsgroup post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post is about a car. It probably was posted to the car-discussion newsgroup,  _rec.autos_. We can confirm by printing `newsgroups.target_names[newsgroups.target[0]]`.\n",
    "\n",
    "**Print newsgroup name at index 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The post at index 0 first appeared in the 'rec.autos' group\n"
     ]
    }
   ],
   "source": [
    "origin = newsgroups.target_names[newsgroups.target[0]]\n",
    "print(f\"The post at index 0 first appeared in the '{origin}' group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets dive deeper into our newsgroup dataset by printing out the dataset size.\n",
    "\n",
    "**Counting the number of newsgroup posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 11314 newsgroup posts\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(newsgroups.data)\n",
    "print(f\"Our dataset contains {dataset_size} newsgroup posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains over 11,000 posts. Our goal is to cluster these posts by topic. To do so, we'll need to transform each newsgroup post into TF vector. We will compute these vectors using Scikit-Learn.\n",
    "\n",
    "## Vectorizing Documents Using Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides built-in class for transforming input texts into TF vectors. That class is called `CountVectorizer`. Initializing `CounterVectorizer` will create a `vectorizer` object capable of vectorizing our texts. \n",
    "\n",
    "**Initializing a `CountVectorizer` object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to vectorize the texts stored in the `newsgroups.data` list. All we need to do is run `vectorizer.fit_transform(newsgroups.data)`. The method-call will return the TF matrix corresponding to the vectorized newsgroup posts. \n",
    "\n",
    "**Listing 15. 7. Computing a TF matrix with Scikit-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 108644)\t4\n",
      "  (0, 110106)\t1\n",
      "  (0, 57577)\t2\n",
      "  (0, 24398)\t2\n",
      "  (0, 79534)\t1\n",
      "  (0, 100942)\t1\n",
      "  (0, 37154)\t1\n",
      "  (0, 45141)\t1\n",
      "  (0, 70570)\t1\n",
      "  (0, 78701)\t2\n",
      "  (0, 101084)\t4\n",
      "  (0, 32499)\t4\n",
      "  (0, 92157)\t1\n",
      "  (0, 100827)\t6\n",
      "  (0, 79461)\t1\n",
      "  (0, 39275)\t1\n",
      "  (0, 60326)\t2\n",
      "  (0, 42332)\t1\n",
      "  (0, 96432)\t1\n",
      "  (0, 67137)\t1\n",
      "  (0, 101732)\t1\n",
      "  (0, 27703)\t1\n",
      "  (0, 49871)\t2\n",
      "  (0, 65338)\t1\n",
      "  (0, 14106)\t1\n",
      "  :\t:\n",
      "  (11313, 55901)\t1\n",
      "  (11313, 93448)\t1\n",
      "  (11313, 97535)\t1\n",
      "  (11313, 93393)\t1\n",
      "  (11313, 109366)\t1\n",
      "  (11313, 102215)\t1\n",
      "  (11313, 29148)\t1\n",
      "  (11313, 26901)\t1\n",
      "  (11313, 94401)\t1\n",
      "  (11313, 89686)\t1\n",
      "  (11313, 80827)\t1\n",
      "  (11313, 72219)\t1\n",
      "  (11313, 32984)\t1\n",
      "  (11313, 82912)\t1\n",
      "  (11313, 99934)\t1\n",
      "  (11313, 96505)\t1\n",
      "  (11313, 72102)\t1\n",
      "  (11313, 32981)\t1\n",
      "  (11313, 82692)\t1\n",
      "  (11313, 101854)\t1\n",
      "  (11313, 66399)\t1\n",
      "  (11313, 63405)\t1\n",
      "  (11313, 61366)\t1\n",
      "  (11313, 7462)\t1\n",
      "  (11313, 109600)\t1\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our printed `tf_matrix` does not appear to be a NumPy array. What sort of data structure is it? We can check, by printing `type(tf_matrix)`.\n",
    "\n",
    "**Checking the data-type of `tf_matrix`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is SciPy object called `csr_matrix`. **CSR** stands for **Compressed Sparse Row**, which is a storage format for compressing matrices that are composed mostly of zeros. These mostly empty matrices are referred to as **sparse matrices**. They can be made smaller by storing only the non-zero elements. This compression leads to more efficient memory usage, and also faster computation. \n",
    "\n",
    "This interplay between various matrix types is useful, but also a bit confusing. In order to minimize confusion, we will now convert `tf_matrix` into a 2D NumPy array.\n",
    "\n",
    "**Converting a CSR matrix to a NumPy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "print(tf_np_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed matrix is a 2D NumPy array. The matrix columns represent posts. Thus, the total column-count equals our dataset’s vocabulary size. \n",
    "\n",
    "**Checking the vocabulary size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our collection of 11314 newsgroup posts contain a total of 114751 unique words\n"
     ]
    }
   ],
   "source": [
    "assert tf_np_matrix.shape == tf_matrix.shape\n",
    "num_posts, vocabulary_size = tf_np_matrix.shape\n",
    "print(f\"Our collection of {num_posts} newsgroup posts contain a total of \" \n",
    "      f\"{vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 114,000 unique words. However, most posts will hold only a few dozen of these words. We can measure the unique word-count of a post at index `i` by counting the number of non-zero elements in row `tf_np_matrix[i]`.  \n",
    "\n",
    "**Counting the unique words in the car post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newsgroup in row 0 contains 64 unique words.\n",
      "The actual word-counts map to the following column indices:\n",
      "\n",
      "[ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n",
      "  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n",
      "  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n",
      "  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n",
      "  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n",
      "  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n",
      " 109294 110106 112936 113262]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "num_unique_words = non_zero_indices.size\n",
    "print(f\"The newsgroup in row 0 contains {num_unique_words} unique words.\")\n",
    "print(\"The actual word-counts map to the following column indices:\\n\")\n",
    "print(non_zero_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first newsgroup post contains 64 unique words.  What are these words? In order to find out, we’ll need a mapping between TF vector indices and word-values. That mapping can be generated by calling `vectorizer.get_feature_names()`. \n",
    "\n",
    "**Printing the unique words in the car post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body', 'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know', 'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name', 'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where', 'wondering', 'years', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names_out()\n",
    "unique_words = [words[i] for i in non_zero_indices]\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve printed all the words in `newsgroups.data[0]`. Of course, not all these words have equal mention-counts. Some words occur more frequently than others.  Lets print the 10 most frequent words within the post, along with their associated counts. \n",
    "\n",
    "**Printing the most frequent words in the car post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word  Count\n",
      "   the      6\n",
      "  this      4\n",
      "   was      4\n",
      "   car      4\n",
      "    if      2\n",
      "    is      2\n",
      "    it      2\n",
      "  from      2\n",
      "    on      2\n",
      "anyone      2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'Word': unique_words, \n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four of the 64 words within the post are mentioned at-least 4 times. One of these four words is _car_. The other three words, however, have nothing to do with cars. These words, _the_, _this_, and _was_, are among the most common words in the English language. NLP practitioners refer to such common words as **stop words**. Running `CountVectorizer(stopwords='english')` remove all stop words during vectorization.\n",
    "\n",
    "**Removing stop words during vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "assert tf_matrix.shape[1] < 114751\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for common_word in ['the', 'this', 'was', 'if', 'it', 'on']:\n",
    "    assert common_word not in words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stop words have been deleted from the recomputed `tf_matrix`. Now, we can re-generate the 10 most frequent words in `newsgroups.data[0]`. \n",
    "\n",
    "**Re-printing the top words after stop-word deletion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stop-word deletion, 34 unique words remain.\n",
      "The 10 most frequent words are:\n",
      "\n",
      "      Word  Count\n",
      "       car      4\n",
      "       60s      1\n",
      "       saw      1\n",
      "   looking      1\n",
      "      mail      1\n",
      "     model      1\n",
      "production      1\n",
      "    really      1\n",
      "      rest      1\n",
      "  separate      1\n"
     ]
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "unique_words = [words[index] for index in non_zero_indices]\n",
    "data = {'Word': unique_words, \n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(f\"After stop-word deletion, {df.shape[0]} unique words remain.\")\n",
    "print(\"The 10 most frequent words are:\\n\")\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stop-word filtering, 34 words remain. Among them, _car_ is the only word that is mentioned more than once. The other 33 words share a mention-count of one. However, not all words are equal in their relevancy. For instance, the word _model_ refers to a car-model. Meanwhile, the word _really_ is more general.  Thus, _really_ is  less relevant than _model_, because the former is mentioned more posts.  Therefore, when ranking words by relevance, we should leverage both post-frequency and count. \n",
    "\n",
    "## Ranking Words by Both Post-Frequency and Count\n",
    "\n",
    "Each of the 34 words in `df.Word` appears in a certain fraction of newsgroup posts. In NLP, this fraction is referred to as the **document frequency** of a word. Our goal is to compute 34 document frequencies, in order to improve our word-relevancy rankings. First, we’ll want to select those columns of `tf_np_matrix` that correspond to the 34 non-zero indices within the `non_zero_indices` array.\n",
    "\n",
    "**Filtering matrix columns with `non_zero_indices`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained a sub-matrix correspond to the 34 words within post 0. The first row of the sub-matrix is:\n",
      "[1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "sub_matrix = tf_np_matrix[:,non_zero_indices]\n",
    "print(\"We obtained a sub-matrix correspond to the 34 words within post 0. \"\n",
    "      \"The first row of the sub-matrix is:\")\n",
    "print(sub_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of `sub_matrix` corresponds to the 34 word-counts in `df`. Together, all the matrix rows correspond to counts across all posts. We’ll need to convert these counts into binary values. \n",
    "\n",
    "**Converting word-counts to binary values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "binary_matrix = binarize(sub_matrix)\n",
    "print(binary_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’ll need to add together the rows of our binary sub-matrix. This will produce a vector of integer counts. Each ith vector element will equal the number of unique posts in which word `i` is present. \n",
    "\n",
    "**Summing matrix rows to obtain post counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This vector counts the unique posts in which each word is mentioned:\n",
      " [  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
      "    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
      "  574   95   98    2  295 1174]\n"
     ]
    }
   ],
   "source": [
    "unique_post_mentions = binary_matrix.sum(axis=0)\n",
    "print(\"This vector counts the unique posts in which each word is \"\n",
    "      f\"mentioned:\\n {unique_post_mentions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that the above 3 procedures can be combined into a single line of code, by running `binarize(tf_np_matrix[:,non_zero_indices]).sum(axis=0)`. Furthermore, substituting NumPy’s `tf_np_matrix` with SciPy’s `tf_matrix` will still produce the same post-instance counts.\n",
    "\n",
    "**Computing post mention-counts in a single line of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy matrix-generated counts:\n",
      " [  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
      "    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
      "  574   95   98    2  295 1174]\n",
      "\n",
      "CSR matrix-generated counts:\n",
      " [[  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
      "     7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
      "   574   95   98    2  295 1174]]\n"
     ]
    }
   ],
   "source": [
    "np_post_mentions = binarize(tf_np_matrix[:,non_zero_indices]).sum(axis=0)\n",
    "csr_post_mentions = binarize(tf_matrix[:,non_zero_indices]).sum(axis=0)\n",
    "print(f'NumPy matrix-generated counts:\\n {np_post_mentions}\\n')\n",
    "print(f'CSR matrix-generated counts:\\n {csr_post_mentions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the printed vector of post-mentions, we know that some words appear in thousands of posts. Other words appear in less than a dozen posts. Lets transform these counts into document frequencies, and align the frequencies with `df.Word`. Afterwards, we’ll output all the words that are mentioned in at-least 10% of newsgroup posts. \n",
    "\n",
    "**Printing the words with the highest document frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word  Count  Document Frequency\n",
      "  know      1            0.273378\n",
      "really      1            0.131960\n",
      " years      1            0.103765\n"
     ]
    }
   ],
   "source": [
    "document_frequencies = unique_post_mentions / dataset_size\n",
    "data = {'Word': unique_words, \n",
    "        'Count': tf_vector[non_zero_indices],\n",
    "        'Document Frequency': document_frequencies}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_common_words = df[df['Document Frequency'] >= .1]\n",
    "print(df_common_words.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the 34 words appear have document frequency that’s greater than 0.1. As expected, these words are very general, and not car-specific. We thus can utilize document frequencies for ranking purposes.\n",
    "\n",
    "**Ranking words by both count and document frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Word  Count  Document Frequency\n",
      "       car      4            0.047375\n",
      "    tellme      1            0.000177\n",
      "  bricklin      1            0.000354\n",
      "     funky      1            0.000619\n",
      "       60s      1            0.001591\n",
      "       70s      1            0.001856\n",
      " enlighten      1            0.002210\n",
      "    bumper      1            0.002298\n",
      "     doors      1            0.005922\n",
      "production      1            0.008397\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(['Count', 'Document Frequency'], \n",
    "                           ascending=[False, True])\n",
    "print(df_sorted[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sorting was successful. New car-related words, such as _bumper_, are now present in our list of top-ranked words.  Lets combine the word-counts and the document frequencies into a single score. We’ll start by computing `1/document_frequencies`. This will produce an array of **inverse document frequencies**, a term that’s commonly shortened to **IDF**. Next, we’ll multiply `df.Word` by the IDF array, in order to compute the combined score. \n",
    "\n",
    "**Combining counts and frequencies into a single score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Word  Count  Document Frequency         IDF    Combined\n",
      "    tellme      1            0.000177 5657.000000 5657.000000\n",
      "  bricklin      1            0.000354 2828.500000 2828.500000\n",
      "     funky      1            0.000619 1616.285714 1616.285714\n",
      "       60s      1            0.001591  628.555556  628.555556\n",
      "       70s      1            0.001856  538.761905  538.761905\n",
      " enlighten      1            0.002210  452.560000  452.560000\n",
      "    bumper      1            0.002298  435.153846  435.153846\n",
      "     doors      1            0.005922  168.865672  168.865672\n",
      "     specs      1            0.008397  119.094737  119.094737\n",
      "production      1            0.008397  119.094737  119.094737\n"
     ]
    }
   ],
   "source": [
    "inverse_document_frequencies = 1 / document_frequencies\n",
    "df['IDF'] = inverse_document_frequencies\n",
    "df['Combined'] = df.Count * inverse_document_frequencies\n",
    "df_sorted = df.sort_values('Combined', ascending=False)\n",
    "print(df_sorted[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new ranking has failed! The word _car_ no longer appears at the top of the list. There is a problem with the IDF values: some of them are huge! Thus, when we multiply word-counts by TDFIF values, the TFIDF will dominate. We thus need to somehow make our TFIDF values smaller. One way to shrink the values down is to apply a logarithmic function. For instance, running `np.log10(1000000)` will return `6`. \n",
    "\n",
    "**Shrinking a large value using its logarithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.log10(1000000) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets recompute our ranking score by running `df.Count * np.log10(df.IDF)`. The product of the counts and the shrunken IDF values should lead to a more reasonable ranking metric.\n",
    "\n",
    "**Adjusting the combined score using logarithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word  Count  Document Frequency         IDF  Combined\n",
      "      car      4            0.047375   21.108209  5.297806\n",
      "   tellme      1            0.000177 5657.000000  3.752586\n",
      " bricklin      1            0.000354 2828.500000  3.451556\n",
      "    funky      1            0.000619 1616.285714  3.208518\n",
      "      60s      1            0.001591  628.555556  2.798344\n",
      "      70s      1            0.001856  538.761905  2.731397\n",
      "enlighten      1            0.002210  452.560000  2.655676\n",
      "   bumper      1            0.002298  435.153846  2.638643\n",
      "    doors      1            0.005922  168.865672  2.227541\n",
      "    specs      1            0.008397  119.094737  2.075893\n"
     ]
    }
   ],
   "source": [
    "df['Combined'] = df.Count * np.log10(df.IDF)\n",
    "df_sorted = df.sort_values('Combined', ascending=False)\n",
    "print(df_sorted[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our adjusted ranking score has yielded good results. The word _car_ is once again present at the top of the ranked list. Also, _bumper_ still appears amongst the top 10 ranked words. Meanwhile, _really_ is missing from the list. \n",
    "\n",
    "Our effective score is called the **term frequency-inverse document frequency**, or **TFIDF** for short. TFIDF is a simple but powerful metric for ranking words within a document. Furthermore, it can also be utilized to vectorize words within a document. Within larger text datasets, TFIDF vectors provide a greater signal of textual similarity and divergence.  That is why Scikit-Learn provides with a built-in class for TFIDF vector computation.\n",
    "\n",
    "\n",
    "### Computing TFIDF Vectors with Scikit-Learn\n",
    "\n",
    "That `TfidfVectorizer` class is nearly identical to `CounterVectorizer`, except that it takes IDF into account during the vectorization process. \n",
    "\n",
    "**LComputing a TFIDF matrix with Scikit-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)\n",
    "assert tfidf_matrix.shape == tf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `tfdif_vectorizer` has learned the same vocabulary as the simpler TF vectorizer. In fact, the indices of words in `tfidf_matrix` are identical to those of `tf_matrix`. \n",
    "\n",
    "**Confirming the preservation of vectorized word indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert tfidf_vectorizer.get_feature_names_out() == words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since word-order is preserved, we should expect the non-zero indices of `tfidf_matrix[0]` to equal our previously computed `non_zero_indices` array. \n",
    "\n",
    "**Confirming the preservation of non-zero indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_np_matrix = tfidf_matrix.toarray()\n",
    "tfidf_vector = tfidf_np_matrix[0]\n",
    "tfidf_non_zero_indices = np.flatnonzero(tfidf_vector)\n",
    "assert np.array_equal(tfidf_non_zero_indices,\n",
    "                      non_zero_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-zero indices of `tf_vector` and `tfidif_vector` are identical. We thus can add the TFIDF vector as a column in our existing `df` table. Adding a _TFIDF_ column will allow us to compare Scikit-Learn’s output with our manually-computed score. \n",
    "\n",
    "**Adding a TFIDF vector to the existing Pandas table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TFIDF'] = tfidf_vector[non_zero_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by `df.TFIDF` should produce a relevance ranking that is consistent with our previous observations. Lets verify that both `df.TFIDF` and `df.Combined` produce the same word-rankings after sorting.\n",
    "\n",
    "**Sorting words by `df.TFIDF`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word  Count  Document Frequency         IDF  Combined    TFIDF\n",
      "      car      4            0.047375   21.108209  5.297806 0.459552\n",
      "   tellme      1            0.000177 5657.000000  3.752586 0.262118\n",
      " bricklin      1            0.000354 2828.500000  3.451556 0.247619\n",
      "    funky      1            0.000619 1616.285714  3.208518 0.234280\n",
      "      60s      1            0.001591  628.555556  2.798344 0.209729\n",
      "      70s      1            0.001856  538.761905  2.731397 0.205568\n",
      "enlighten      1            0.002210  452.560000  2.655676 0.200827\n",
      "   bumper      1            0.002298  435.153846  2.638643 0.199756\n",
      "    doors      1            0.005922  168.865672  2.227541 0.173540\n",
      "    specs      1            0.008397  119.094737  2.075893 0.163752\n"
     ]
    }
   ],
   "source": [
    "df_sorted_old = df.sort_values('Combined', ascending=False)\n",
    "df_sorted_new = df.sort_values('TFIDF', ascending=False)\n",
    "assert np.array_equal(df_sorted_old['Word'].values,\n",
    "                      df_sorted_new['Word'].values)\n",
    "print(df_sorted_new[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our word-rankings have remained unchanged. However, the values of the _TFIDF_ and _Combined_ columns are not identical. As it turns out, Scikit-Learn automatically normalizes its TFIDF vector results. The magnitude of `df.TFIDF` has been modified to equal 1. \n",
    "\n",
    "**Listing 15. 30. Confirming that our TFIDF vector is normalized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "assert norm(df.TFIDF.values) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would Scikit-Learn automatically normalize the vectors? For our own benefit! As discussed in Section Thirteen, its easier to compute text-vector similarity when all vector magnitudes equal 1. Consequently, our normalized TFIDF matrix is primed for similarity analysis.\n",
    "\n",
    "## 15.4. Computing Similarities Across Large Document Datasets\n",
    "\n",
    "Which of our newsgroup posts is most similar to `newsgroups.post[0]`? We can obtain the answer by computing all the cosine similarities between `tfidf_np_matrix` and `tf_np_matrix[0]`.\n",
    "\n",
    "**Listing 15. 31. Computing similarities to a single newsgroup post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = tfidf_np_matrix @ tfidf_np_matrix[0]\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `cosine_similarities[0]` is equal to 1.0. This is not surprising, since `newsgroups_data[0]` will have a perfect similarity with itself. What is the next-highest similarity in the vector? We can find out by calling `np.argsort(cosine_similarities)[-2]`. \n",
    "\n",
    "**Listing 15. 32. Finding the most similar newsgroup post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_index = np.argsort(cosine_similarities)[-2]\n",
    "similarity = cosine_similarities[most_similar_index]\n",
    "most_similar_post = newsgroups.data[most_similar_index]\n",
    "print(f\"The following post has a cosine similarity of {similarity:.2f} \"\n",
    "       \"with newsgroups.data[0]:\\n\")\n",
    "print(most_similar_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we compute the matrix of all-by-all cosine similarities? The naive situation is to multiply `tfidf_np_matrix` with its transpose. However, for reasons discussed in Section Thirteen, this matrix multiplication is not computationally efficient. We need to reduce the matrix size, using `TruncatedSVD`. \n",
    "\n",
    "**Listing 15. 33. Dimensionally reducing `tfidf_matrix` using SVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "shrunk_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)\n",
    "print(f\"We've dimensionally-reduced a {tfidf_matrix.shape[1]}-column \"\n",
    "      f\"{type(tfidf_matrix)} matrix.\")\n",
    "print(f\"Our output is a {shrunk_matrix.shape[1]}-column \"\n",
    "      f\"{type(shrunk_matrix)} matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our shrunk matrix contains just 100 columns. We can now efficiently compute the cosine similarities by running `shrunk_matrix @ shrunk_matrix.T`. However, first we’ll need to confirm that the matrix rows remain normalized.\n",
    "\n",
    "**Listing 15. 34. Checking the magnitude of `shrunk_matrix[0]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude = norm(shrunk_matrix[0])\n",
    "print(f\"The magnitude of the first row is {magnitude:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of the row is less than 1. Scikit-Learn’s SVD output has not been automatically normalized. We’ll need to manually normalize the matrix, prior to computing the similarities.\n",
    "\n",
    "**Listing 15. 35. Normalizing the SVD output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "shrunk_norm_matrix = normalize(shrunk_matrix)\n",
    "magnitude = norm(shrunk_norm_matrix[0])\n",
    "print(f\"The magnitude of the first row is {magnitude:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shrunken matrix has been normalized. Now, running `shrunk_norm_matrix @ shrunk_norm_matrix.T` should produce a matrix of all-by-all cosine similarities.\n",
    "\n",
    "**Listing 15. 36. Computing all-by-all cosine similarities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix = shrunk_norm_matrix @ shrunk_norm_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our similarity matrix. Lets leverage it to choose a random pair of very similar texts.  \n",
    "\n",
    "**Listing 15. 37. Choosing a random pair of similar posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "index1 = np.random.randint(dataset_size)\n",
    "index2 = np.argsort(cosine_similarity_matrix[index1])[-2]\n",
    "similarity = cosine_similarity_matrix[index1][index2]\n",
    "print(f\"The posts at indices {index1} and {index2} share a cosine \"\n",
    "      f\"similarity of {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two selected indices share a very high cosine similarity of 0.91. The corresponding posts must be exceedingly similar. Lets confirm. First, we’ll print the post at `index2`.\n",
    "\n",
    "**Listing 15. 38. Printing a randomly chosen post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newsgroups.data[index2].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the printed post is a question. Its safe to assume that the post at `index1` is an answer to that question.\n",
    "\n",
    "**Listing 15. 39. Printing the most-similar post response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newsgroups.data[index1].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have examined two pairs of similar posts. Lets challenge ourselves to find something more interesting. We’ll search for clusters of similar texts, where posts within a cluster share some text without perfectly overlapping. \n",
    "\n",
    "## Clustering Texts by Topic\n",
    "\n",
    "In Section Ten, we introduced two clustering algorithms; K-means and DBSCAN. K-means can only cluster on Euclidean distance. Conversely, DBSCAN, can cluster based on any distance metric. One possible metric is cosine distance which equals 1 minus cosine similarity. Lets cluster `shrunk_matrix` with BDSCAN, using the cosine distance. We'll set the `eps` and `min_samples` parameters to 0.4 and 50, respectively.\n",
    "\n",
    "**Listing 15. 40. Clustering newsgroup posts with DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "cluster_model = DBSCAN(eps=0.4, min_samples=50, metric='cosine')\n",
    "clusters = cluster_model.fit_predict(shrunk_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W can expect our `clusters` array to contain somewhere between 10 and 25 clusters. Otherwise, there’s something wrong with our input clustering parameters. We’ll now proceed to count the number of clusters.\n",
    "**Listing 15. 41. Counting the number of DBSCAN clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = clusters.max() + 1\n",
    "print(f\"We've generated {cluster_count} DBSCAN clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve generated just 3 clusters, which is way lower than our expected cluster count. Clearly, our DBSCAN parameters were wrong. Unfortunately, DBSCAN lacks a well-established algorithm for optimizing these two crucial parameters.\n",
    "\n",
    "K-means, on the other hand, takes as input a single K parameter. However, the K-means algorithm can only cluster based on Euclidean distance.  With basic algebra, we can easily show that the Euclidean distance of two normalized vectors is proportional to the square-root of the cosine distance. This relationship provides us with mathematical justification for clustering `shrunk_norm_matrix` using K-means. \n",
    "\n",
    "We’ll want to estimate the true value of K by generating an Elbow plot. To this end, we’ll execute K-means across K values of 1 through 60, and afterwards plot the inertia results. Running Scikit-Learn's implementation of the **Mini Batch K-means** algorithm will produce this plot much faster than regular K-means.\n",
    "\n",
    "**Listing 15. 42. Comparing `KMeans` to `MiniBatchKMeans`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "import time \n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "k=20\n",
    "times = []\n",
    "for KMeans_class in [KMeans, MiniBatchKMeans]:\n",
    "    start_time = time.time()\n",
    "    KMeans_class(k).fit(shrunk_norm_matrix)\n",
    "    times.append(time.time() - start_time)\n",
    "\n",
    "running_time_ratio = times[0] / times[1]\n",
    "print(f\"Mini Batch K-means ran {running_time_ratio:.2f} times faster \"\n",
    "       \"than regular K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MiniBatchKMeans` runs approximately  10x faster than regular `KMeans`.  We’ll now proceed to generate the Elbow plot, using Mini Batch K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_values = range(1, 61)\n",
    "inertia_values = [MiniBatchKMeans(k).fit(shrunk_norm_matrix).inertia_ \n",
    "                  for k in k_values]\n",
    "plt.plot(k_values, inertia_values)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.axvline(20, c='k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plotted curve decreases smoothly. The precise location of a bent elbow-shaped transition is difficult to spot. We do see that the curve is noticeably more steep when K is less than 20. Somewhere after 20 clusters, the curve begins to flatten out. We can infer that 20 is a reasonable estimate of the K parameter. Our solution isn’t perfect, but it’s feasible. Sometimes, when we deal with real-world data, a feasible solution is the best that we can expect.\n",
    "\n",
    "We will now divide `shrunk_norm_matrix` into 20 clusters.\n",
    "\n",
    "**Listing 15. 44. Clustering newsgroup posts into 20 clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "cluster_model = KMeans(n_clusters=20)\n",
    "clusters = cluster_model.fit_predict(shrunk_norm_matrix)\n",
    "df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that've clustered our texts, lets explore the cluster contents. We'll begin by analyzing a single cluster. Later, we'll proceed to analyze all clusters simultaneously.\n",
    "\n",
    "### 15.5.1. Exploring a Single Text Cluster\n",
    " \n",
    "One of our 20 clusters contains the car-post at index 0 of `newsgroups.data`. Lets isolate and count the number of texts that group together with that car-themed message.\n",
    "\n",
    "**Listing 15. 45. Isolating the car cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_car = df[df.Cluster == clusters[0]]\n",
    "cluster_size = df_car.shape[0]\n",
    "print(f\"{cluster_size} posts cluster together with the car-themed post \"\n",
    "       \"at index 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: The contents of the cluster might differ slightly on your local machine. The total cluster size might minimally diverge from 393. If this happens, then the subsequent sequence of code listings might produce different results. Regardless of these differences, you should still be able to draw similar conclusions from your locally generated outputs.**\n",
    "\n",
    "393 posts cluster with the car-themed texts at index 0. Presumably, these posts are also about cars. If so, then a randomly chosen post should mention an automobile. \n",
    "\n",
    "**Listing 15. 46. Printing a random post in the car cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "def get_post_category(index):\n",
    "    target_index = newsgroups.target[index]\n",
    "    return newsgroups.target_names[target_index]\n",
    "\n",
    "random_index = np.random.choice(df_car.Index.values)\n",
    "post_category = get_post_category(random_index)\n",
    "\n",
    "print(f\"This post appeared in the {post_category} discussion group:\\n\")\n",
    "print(newsgroups.data[random_index].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post belongs in the _rec.autos_ discussion group. How many of the nearly 400 posts within the cluster belong to _rec.autos_? Lets find out.\n",
    "\n",
    "**Listing 15. 47. Checking cluster membership to _rec.autos_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_autos_count = 0\n",
    "for index in df_car.Index.values:\n",
    "    if get_post_category(index) == 'rec.autos':\n",
    "        rec_autos_count += 1\n",
    "        \n",
    "rec_autos_percent = 100 * rec_autos_count / cluster_size\n",
    "print(f\"{rec_autos_percent:.2f}% of posts within the cluster appeared \"\n",
    "       \"in the rec.autos discussion group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84% of the posts in within the cluster appeared in _rec.autos_. What about the remaining 16% of clustered posts? Lets isolate the indices of posts in `df_car` that do not belong to _rec.autos_. Afterwards, we’ll choose a random index and then print the associated post.\n",
    "\n",
    "**Listing 15. 48. Examining a post that did not appear in rec.autos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "not_autos_indices = [index for index in df_car.Index.values\n",
    "                     if get_post_category(index) != 'rec.autos']\n",
    "\n",
    "random_index = np.random.choice(not_autos_indices)\n",
    "post_category = get_post_category(random_index)\n",
    "\n",
    "print(f\"This post appeared in the {post_category} discussion group:\\n\")\n",
    "print(newsgroups.data[random_index].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thematically, the post is about automobiles.  Thus, it appears to have clustered correctly. What about the other 60-or-so posts represented by the `not_autos_indices` list? How do we evaluate their relevance? Well, we can aggregate their content by displaying the top-ranking words across all posts. We’ll rank each word by summing its TFIDF across each index in `not_autos_indices`. \n",
    "\n",
    "**Listing 15. 49. Ranking the top 10 words with TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_words_by_tfidf(indices):\n",
    "    summed_tfidf = np.asarray(tfidf_matrix[indices].sum(axis=0))[0]\n",
    "    data = {'Word': words,\n",
    "            'Summed TFIDF': summed_tfidf}\n",
    "    return pd.DataFrame(data).sort_values('Summed TFIDF', ascending=False)\n",
    "\n",
    "df_ranked_words = rank_words_by_tfidf(not_autos_indices)\n",
    "print(df_ranked_words[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two top-ranking words are _car_ and _cars_. Elsewhere on ranked list, we see mentions of _radar_, _odometer_, and _speed_. How do these speed-themed keywords compare with the rest of the posts in the car cluster? We can check, by inputting `df_car.Index.values` into `rank_words_by_tfidf`.\n",
    "\n",
    "**Listing 15. 50. Ranking the top 10 words in the car cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranked_words = rank_words_by_tfidf(df_car.Index.values)\n",
    "print(df_ranked_words[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the posts in the`df_car` cluster focus on car-engines and car-dealers. However, a minority of the posts discuss radar measurements of car speed. These radar posts are more likely to appear in the _sci.electronics_ newsgroup. Nonetheless, these posts legitimately discuss cars (as opposed to discussing politics, software or medicine). Thus, our `df_car` cluster appears to be genuine. \n",
    "\n",
    "In this same manner, we can utilize `rank_words_by_tfidf` to get the top keywords for each of the 20 clusters. We can visualize these cluster-keywords as images within a single coherent plot. \n",
    "\n",
    "## Visualizing Text Clusters\n",
    "\n",
    "Lets visualize the words within our car-cluster in a 2D grid, where word-size is proportional to significance. This type of visualization is called a **word cloud**. We'll utilize `plt.text` to generate a word cloud of the top words  in `df_ranked_words`. \n",
    "\n",
    "**Listing 15. 51. Plotting a word cloud with Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x_coord in np.arange(0, 1, .2):\n",
    "    for y_coord in np.arange(0, 1, .2):\n",
    "        word, significance = df_ranked_words.iloc[i].values        \n",
    "        plt.text(y_coord, x_coord, word, fontsize=2*significance)\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our visualization is a mess!  We need to plot our words much more intelligently. This can be done using the external Wordcloud library. The library is able to generate word clouds in a manner that’s visually appealing. Lets import and initialize the library’s `WordCloud` class. \n",
    "\n",
    "**Listing 15. 52. Initializing the `WordCloud` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "cloud_generator = WordCloud(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `WordCloud()` returns a `cloud_generator` object. We’ll use the object’s `fit_words` method to generate an image. We’ll store that word-cloud image within a `wordcloud_image` variable. However, we won’t plot the image just yet.\n",
    "\n",
    "**Listing 15. 53. Generating a word-cloud image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_score = {word: score \n",
    "                  for word, score in df_ranked_words[:10].values}\n",
    "wordcloud_image = cloud_generator.fit_words(words_to_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’re ready to visualize `wordcloud_image`. Running `plt.imshow(wordcloud_image)` will display our generated word cloud. \n",
    "\n",
    "**Listing 15. 54. Plotting an image using `plt.imshow`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image's dark background makes it hard to read the words. We can change the background from black to white by running `WordCloud(background_color='white')`. Also, the edges of the individual letters are pixelated and blocky. We can smooth all edges in our image plot by passing `interplolation=\"bilinear\"` into `plt.imshow`.\n",
    "\n",
    "**Listing 15. 55. Improving the word-cloud image quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_generator = WordCloud(background_color='white',\n",
    "                            random_state=1)\n",
    "wordcloud_image = cloud_generator.fit_words(words_to_score)\n",
    "plt.imshow(wordcloud_image, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top words the car-cluster have been successfully visualized. Now, lets apply word-cloud visualization to some other randomly chosen cluster. \n",
    "\n",
    "**Listing 15. 56. Plotting a word cloud for a random cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def cluster_to_image(df_cluster, max_words=15, **kwargs):\n",
    "    indices = df_cluster.Index.values\n",
    "    df_ranked_words = rank_words_by_tfidf(indices)[:max_words]\n",
    "    words_to_score = {word: score \n",
    "                      for word, score in df_ranked_words[:max_words].values}\n",
    "    cloud_generator = WordCloud(background_color='white',\n",
    "                                color_func=_color_func,\n",
    "                                random_state=1)\n",
    "    wordcloud_image = cloud_generator.fit_words(words_to_score)\n",
    "    return wordcloud_image\n",
    "\n",
    "def _color_func(*args, **kwargs):\n",
    "    return np.random.choice(['black', 'blue', 'teal', 'purple', 'brown'])\n",
    "\n",
    "cluster_id = np.random.randint(0, 20)\n",
    "df_random_cluster = df[df.Cluster == cluster_id]\n",
    "wordcloud_image = cluster_to_image(df_random_cluster)\n",
    "plt.imshow(wordcloud_image, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: The contents and the order of the clusters might differ slightly on your local machine. If this occurs, then the randomly chosen cluster will not reflect what is described within these notes.**\n",
    "\n",
    "Our randomly chosen cluster includes top words  such as _monitor_, _video_, _memory_, _card_, _motherboard_, _bit_, and _ram_. The cluster seems to focus on technology and computer hardware. We can verify by printing the most-common newsgroup category within the cluster.\n",
    "\n",
    "**Listing 15. 57. Checking the most-common cluster category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_top_category(df_cluster):\n",
    "    categories = [get_post_category(index)\n",
    "                  for index in df_cluster.Index.values]\n",
    "    top_category, _ = Counter(categories).most_common()[0]\n",
    "    return top_category\n",
    "\n",
    "top_category = get_top_category(df_random_cluster)\n",
    "print(\"The posts within the cluster commonly appear in the \"\n",
    "      f\"'{top_category}' newsgroup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W've successfully identified the cluster's topic simply by looking at the word cloud.\n",
    "\n",
    "So far, we’ve generated two separate word clouds for two distinct clusters. However, our end-goal is to display multiple word clouds simultaneously. We’ll now proceed to visualize all word clouds within a single figure, using a Matplotlib concept called a subplot.\n",
    "\n",
    "\n",
    "### 15.6.1. Using Subplots to Display Multiple Word Clouds\n",
    "\n",
    "Matplotlib allows us to include multiple plots within a single figure. Each distinct plot is called a **subplot**. \\ We can create a sublot-grid containing `r` rows and `c` columns by running `plt.subplots(r, c)`. Below, we’ll generate 2x2 grid of subplots by running `plt.subplots(2, 2)`.  For every unique subplot positioned at `(r, c`), we’ll plot a quadratic curve in which `y` equals `r * x*x + c * x`.\n",
    "\n",
    "**Listing 15. 58. Generating 4 subplots using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 2)\n",
    "for r in range(2):\n",
    "    for c in range(2):\n",
    "        x = np.arange(0, 1, .2)\n",
    "        y = r * x * x + c * x \n",
    "        axes[r][c].plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four different curves appear within the subplots of our grid. We can replace any of these curves with a word cloud. Lets visualize `wordcloud_image` in the lower-left quadrant of the grid.\n",
    "\n",
    "**Listing 15. 59. Plotting a word cloud within a subplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 2)\n",
    "for r in range(2):\n",
    "    for c in range(2):\n",
    "        if (r, c) == (1, 0):\n",
    "            axes[r][c].set_title(top_category)\n",
    "            axes[r][c].imshow(wordcloud_image,\n",
    "                              interpolation=\"bilinear\")\n",
    "        else:\n",
    "            x = np.arange(0, 1, .2)\n",
    "            y = r * x * x + c * x \n",
    "        axes[r][c].plot(x, y)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate a figure that’s 10 inches high and 15 inches inches. The large figure will hold 20 subplots, aligned in a 5x4 grid. Each subplot will contain a word cloud corresponding to one of our clusters. Every subplot title will be set to the dominant newsgroup category within a cluster. Also, we’ll include the cluster-index in each title, for later reference. Finally, we’ll remove the axis tick-marks from all plots. The final visualization will give us birds-eye-view of all the dominant word-patterns across all 20 clusters.\n",
    "\n",
    "**Listing 15. 60. Visualizing all clusters using 20 subplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def get_title(df_cluster):\n",
    "    top_category = get_top_category(df_cluster)\n",
    "    cluster_id = df_cluster.Cluster.values[0]\n",
    "    return f\"{cluster_id}: {top_category}\"\n",
    "\n",
    "figure, axes = plt.subplots(5, 4, figsize=(20, 15))\n",
    "cluster_groups = list(df.groupby('Cluster'))\n",
    "for r in range(5):\n",
    "    for c in range(4):\n",
    "        _, df_cluster = cluster_groups.pop(0)\n",
    "        wordcloud_image = cluster_to_image(df_cluster, max_words=10)\n",
    "        ax = axes[r][c]\n",
    "        ax.imshow(wordcloud_image,\n",
    "                  interpolation=\"bilinear\")\n",
    "        ax.set_title(get_title(df_cluster), fontsize=20)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "#plt.savefig('../images/fig15-8.png', dpi=600)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve visualized the top words across all 20 clusters. For the most part, the visualized words make sense! Arguably, 75% of the clusters contain top-words corresponding with their dominant category titles.\n",
    "\n",
    "Of course, there are several issues with our output.For instance, Cluster 7 has a subplot title of `sci.med`, yet is word-cloud is composed of words like _pitt_, _msg_, and _gordon_.  Unfortunately, word-cloud visualization isn’t always perfect. Fortunately, there are steps we can take to salvage indecipherable word-clouds. For instance, we could disregard the top x words within the cluster, and visualize the cloud using the next top-raking words. Below, we will remove the top 10 words from Cluster 7. \n",
    "\n",
    "**Listing 15. 61. Recomputing a word cloud after filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "df_cluster= df[df.Cluster == 7]\n",
    "df_ranked_words = rank_words_by_tfidf(df_cluster.Index.values)\n",
    "\n",
    "words_to_score = {word: score \n",
    "                  for word, score in df_ranked_words[10:25].values}\n",
    "cloud_generator = WordCloud(background_color='white',\n",
    "                            color_func=_color_func,\n",
    "                            random_state=1)\n",
    "wordcloud_image = cloud_generator.fit_words(words_to_score)\n",
    "plt.imshow(wordcloud_image, interpolation=\"bilinear\")\n",
    "plt.title(get_title(df_cluster), fontsize=20)\n",
    "plt.xticks([])\n",
    "plt.yticks([]) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
