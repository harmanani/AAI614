{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5639149e",
   "metadata": {
    "id": "5639149e"
   },
   "source": [
    "# AAI614: Data Science & its Applications\n",
    "\n",
    "*Notebook 8.4: Classifying Sentiments in Arabizi*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI614/blob/main/Week%208/Notebook8.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Please check the following [paper](https://github.com/harmanani/AAI614/blob/main/Week%208/Arabizi.pdf) for reference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5260708",
   "metadata": {
    "id": "b5260708"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, MaxPooling1D\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0626490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0626490",
    "outputId": "35f130d2-8e19-4252-9038-0dc2c0833eb4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Douniakatibi eh ktir bahdaleeeeee</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shu b7bk ya beirut üòç‚ù§Ô∏èüå≤</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Courtesy words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L liste shwe ktir mesh zabta</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@trissaabiissa ktir helo ltalej bs bye7la bo w...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akh ya allah ghachaytine dehek hahahaha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>Lakk ya 1000 halaaa ü§£üòç ehh hayda 7akee ü§£</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>Knt ktir yenzal dam rehet al hospital w kouwyo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>@fadelkaadan anna bi beirut bas raj3anee 3a Mi...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Sma3et sot bel beit mich mestarjeye odhar men ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>ya allah enta w batnak hahahaha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Joke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet sentiment  \\\n",
       "0                    @Douniakatibi eh ktir bahdaleeeeee  Negative   \n",
       "1                              Shu b7bk ya beirut üòç‚ù§Ô∏èüå≤   Positive   \n",
       "2                         L liste shwe ktir mesh zabta   Negative   \n",
       "3     @trissaabiissa ktir helo ltalej bs bye7la bo w...  Positive   \n",
       "4               akh ya allah ghachaytine dehek hahahaha  Positive   \n",
       "...                                                 ...       ...   \n",
       "1195          Lakk ya 1000 halaaa ü§£üòç ehh hayda 7akee ü§£   Positive   \n",
       "1196  Knt ktir yenzal dam rehet al hospital w kouwyo...  Positive   \n",
       "1197  @fadelkaadan anna bi beirut bas raj3anee 3a Mi...  Negative   \n",
       "1198  Sma3et sot bel beit mich mestarjeye odhar men ...  Negative   \n",
       "1199                   ya allah enta w batnak hahahaha   Positive   \n",
       "\n",
       "           highlight  \n",
       "0                NaN  \n",
       "1     Courtesy words  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "1195             NaN  \n",
       "1196             NaN  \n",
       "1197             NaN  \n",
       "1198             NaN  \n",
       "1199            Joke  \n",
       "\n",
       "[1200 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARABIZI_FILE = \"2-class-sentiment-arabizi.csv\"\n",
    "csvfile = pd.read_csv(\"https://raw.githubusercontent.com/harmanani/AAI614/refs/heads/main/Week%208/2-class-sentiment-arabizi.csv\")\n",
    "csvfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4f7f3",
   "metadata": {
    "id": "7ee4f7f3"
   },
   "source": [
    "As you can see, each data point is composed of a tweet, a sentiment label, and a highlight.  The highlight will not be used in this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c838f",
   "metadata": {
    "id": "485c838f"
   },
   "source": [
    "## Defining useful global variables\n",
    "\n",
    "Next you will define some global variables that will be used throughout the assignment. \n",
    "\n",
    "- `NUM_WORDS`: The maximum number of words to keep, based on word frequency. Defaults to 2000.\n",
    "\n",
    "\n",
    "- `EMBEDDING_DIM`: Dimension of the dense embedding, will be used in the embedding layer of the model. Defaults to 16.\n",
    "\n",
    "\n",
    "- `MAXLEN`: Maximum length of all sequences. Defaults to 120.\n",
    "\n",
    "\n",
    "- `PADDING`: Padding strategy (pad either before or after each sequence.). Defaults to 'post'.\n",
    "\n",
    "\n",
    "- `OOV_TOKEN`: Token to replace out-of-vocabulary words during text_to_sequence calls. Defaults to \"\\<OOV>\".\n",
    "\n",
    "    \n",
    "- `TRAINING_SPLIT`: Proportion of data used for training. Defaults to 0.8\n",
    "\n",
    "**For now leave them unchanged but after submitting your assignment for grading you are encouraged to come back here and play with these parameters to see the impact they have in the classification process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "BuGbpXEEsTiw",
   "metadata": {
    "id": "BuGbpXEEsTiw"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 2000\n",
    "EMBEDDING_DIM = 64\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "TRAINING_SPLIT = 0.8\n",
    "MAXLEN = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e9364",
   "metadata": {
    "id": "f25e9364"
   },
   "source": [
    "## Loading and pre-processing the data\n",
    "\n",
    "Now you should code the functions to remove stopwords from text and to load the data from a csv file.\n",
    "\n",
    "Since you already coded these functions for the previous week, these are provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8039b079",
   "metadata": {
    "id": "8039b079"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    # List of stopwords\n",
    "    stopwords = ['chu', 'chou', 'hal', 'chou', 'fikeee', 'eh', 'ma', '3a', '3am', '3an', '3ana', '3endon', '3m', 'aal', '3al','3ala','al', 'ana', \n",
    "                 'b', 'bas', 'bel', 'bi', 'chi', 'el', 'ele', 'enn', 'enno', 'eno', 'mch', '3layki', '3layon',\n",
    "                 'enta', 'enteh', 'eza', 'fi', 'fik', 'fina', 'fine', 'fiya', 'fiye', 'wlik', 'shi',\n",
    "                 'hal', 'hayda', 'hek', 'inta', 'iza', 'kaza', 'kel', 'kente', 'kif', \n",
    "                 'kint', 'kinte', 'l', 'la', 'la2na', 'lal', 'li', 'ma3', 'ma3a', 'ma3ak', \n",
    "                 'ma3e', 'ma3ekk', 'ma3i', 'ma3ik', 'ma3ke', 'ma3na', 'ma3o', 'men', 'mn', \n",
    "                 'n7na', 'shu', 'tab', 'tayeb', 'w', 'wlek', 'ya', 'ya3ne', 'yala', 'yalle']\n",
    "\n",
    "    # Sentence converted to lowercase-only\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    words = sentence.split()\n",
    "    no_words = [w for w in words if w not in stopwords]\n",
    "    sentence = \" \".join(no_words)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9KAaSs1QSBJf",
   "metadata": {
    "id": "9KAaSs1QSBJf"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "\n",
    "    #if isinstance(tweet, float):\n",
    "    #  return \"\"\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = re.sub(r\"h[ha]{3,}\", \"hahaha\", temp)\n",
    "    temp = re.sub(r\"lo{1,}l\", \"lol\", temp)\n",
    "    return temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "Sfz4oZ08SD2a",
   "metadata": {
    "id": "Sfz4oZ08SD2a"
   },
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    sentences = []\n",
    "    sentiments_labels = []\n",
    "    emotions_labels = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            sentiments_labels.append(row[1])\n",
    "            emotions_labels.append(row[2])\n",
    "            sentence = row[0]\n",
    "            sentence = remove_stopwords(sentence)\n",
    "            sentence = clean_tweet(sentence)\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences, sentiments_labels, emotions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565105f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5565105f",
    "outputId": "9cb47153-983e-4278-9dc6-8144b80f975a"
   },
   "outputs": [],
   "source": [
    "# Test the functions\n",
    "sentences, sentiments_labels,  emotions_labels = parse_data_from_file(ARABIZI_FILE)\n",
    " \n",
    "# Shuffle the lists \n",
    "temp = list(zip(sentences, sentiments_labels, emotions_labels))\n",
    "shuffle(temp)\n",
    "\n",
    "res1, res2, res3 = zip(*temp)\n",
    "sentences, sentiments_labels,  emotions_labels= list(res1), list(res2), list(res3)\n",
    "\n",
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"First sentence has the following words: {sentences[0].split()}.\\n\")\n",
    "print(f\"There are {len(emotions_labels)} emotions labels in the dataset.\")\n",
    "print(f\"The first 5 labels are {emotions_labels[:5]}\\n\")\n",
    "print(f\"There are {len(sentiments_labels)} sentiment labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {sentiments_labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b39db",
   "metadata": {
    "id": "011b39db"
   },
   "source": [
    "## Training - Validation Split\n",
    "\n",
    "`train_val_split`, which given the list of sentences, the list of labels and the proportion of data for the training set, returns the training and validation sentences and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ce4e6fc",
   "metadata": {
    "id": "9ce4e6fc"
   },
   "outputs": [],
   "source": [
    "def train_val_split(sentences, labels, training_split):\n",
    "       \n",
    "    # Compute the number of sentences that will be used for training (should be an integer)   \n",
    "    train_size = int(len(sentences) * training_split) # YOUR CODE HERE\n",
    "\n",
    "    # Split the sentences and labels into train/validation splits\n",
    "    train_sentences = sentences[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "\n",
    "    validation_sentences = sentences[train_size:]\n",
    "    validation_labels = labels[train_size:]\n",
    "        \n",
    "    return train_sentences, validation_sentences, train_labels, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9c0f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04c9c0f9",
    "outputId": "e22b654e-5412-47ac-931c-9afe3e3c6ffb"
   },
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, sentiments_labels, TRAINING_SPLIT)\n",
    "\n",
    "# Test your function\n",
    "vocab_size = len(train_sentences)\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "max_length = max([len(x) for x in sentences])\n",
    "MAXLEN = max_length\n",
    "\n",
    "print(f\"The longest tweet is {max_length} characters long!\\n\")\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac98dde",
   "metadata": {
    "id": "3ac98dde"
   },
   "source": [
    "## Tokenization - Sequences and padding\n",
    "\n",
    "Now that you have sets for training and validation it is time for you to begin the tokenization process.\n",
    "\n",
    "Begin by completing the `fit_tokenizer` function below. This function should return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to the training sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e202298",
   "metadata": {
    "id": "2e202298"
   },
   "outputs": [],
   "source": [
    "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    vocab_size = len(train_sentences)\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    max_length = 120\n",
    "        \n",
    "    # Instantiate the Tokenizer class, passing in the correct values for num_words and oov_token\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_tok)\n",
    "    \n",
    "    # Fit the tokenizer to the training sentences\n",
    "    \n",
    "    # Generate the word index dictionary\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "  \n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caa2fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82caa2fa",
    "outputId": "4a191e71-ac47-4cbc-f9eb-72700e158141"
   },
   "outputs": [],
   "source": [
    "# Test your function\n",
    "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450b0d6",
   "metadata": {
    "id": "0450b0d6"
   },
   "source": [
    "Now that the tokenizer has been fitted to the training data, you need a function that will convert each text data point into its padded sequence representation, for this complete the `seq_and_pad` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d19335a",
   "metadata": {
    "id": "9d19335a"
   },
   "outputs": [],
   "source": [
    "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    trunc_type='post'\n",
    "    \n",
    "    # Convert sentences to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # Pad the sequences using the correct padding and maxlen\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=trunc_type)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339bed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5339bed7",
    "outputId": "fc2c86d4-8cb4-4e50-ec27-fdd3f6a58050"
   },
   "outputs": [],
   "source": [
    "train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(val_sentences, tokenizer, PADDING, MAXLEN)\n",
    "\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599d0dc",
   "metadata": {
    "id": "0599d0dc"
   },
   "source": [
    "Finally you need to tokenize the labels. For this complete the `tokenize_labels` function below.\n",
    "\n",
    "A couple of things to note:\n",
    "- You should fit the tokenizer to all the labels to avoid the case of a particular label not being present in the validation set. Since you are dealing with labels there should never be an OOV label.\n",
    "\n",
    "\n",
    "- In the previous function you used the `pad_sequences` function which returns numpy arrays. Here you will not be using it since you don't need to pad the labels so you need to make the conversion to numpy arrays yourself.\n",
    "\n",
    "\n",
    "- The argument `split_labels` refers to the labels of a particular split (train or validation). This is because the function should work independently of the split being used.\n",
    "\n",
    "\n",
    "- Using Keras' Tokenizer yields values that start at 1 rather than at 0. This will present a problem when training since Keras usually expects the labels to start at 0. To work around this issue you could use an extra neuron in the last layer of your model. However this approach is rather hacky and not very clear. Instead you will substract 1 from every value of the labels that the function returns. Remember that when using numpy arrays you can simply do something like `np.array - 1` to accomplish this since numpy allows for vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac9156a2",
   "metadata": {
    "id": "ac9156a2"
   },
   "outputs": [],
   "source": [
    "def tokenize_labels(all_labels, split_labels):\n",
    "       \n",
    "    # Instantiate the Tokenizer (no additional arguments needed)\n",
    "    label_tokenizer = Tokenizer()\n",
    "    \n",
    "    # Fit the tokenizer on all the labels\n",
    "    label_tokenizer.fit_on_texts(all_labels)\n",
    "    \n",
    "    # Convert labels to sequences\n",
    "    label_seq = label_tokenizer.texts_to_sequences(split_labels)\n",
    "    \n",
    "    # Convert sequences to a numpy array. Don't forget to substact 1 from every entry in the array!\n",
    "    label_seq_np = np.array(label_seq)-1\n",
    "        \n",
    "    return label_seq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b911a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97b911a9",
    "outputId": "ac4382e5-6f7a-4e20-c885-3c68022e6226"
   },
   "outputs": [],
   "source": [
    "train_label_seq = tokenize_labels(sentiments_labels, train_labels)\n",
    "val_label_seq = tokenize_labels(sentiments_labels, val_labels)\n",
    "\n",
    "print(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:5]}\\n\")\n",
    "print(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\n",
    "print(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\n",
    "print(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a3ba2",
   "metadata": {
    "id": "409a3ba2"
   },
   "source": [
    "## Selecting the model for text classification\n",
    "\n",
    "Now that the data is ready to be fed a Neural Network it is time for you to define the model that will classify each text as being part of a certain category. \n",
    "\n",
    "For this complete the `create_model` below. \n",
    "\n",
    "A couple of things to keep in mind:\n",
    "\n",
    "- Notice that this function has three parameters, all of which are meant to be passed to an [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer, which is what you will probably use as a first layer for your model.\n",
    "\n",
    "\n",
    "- The last layer should be a Dense layer with 5 units (since there are 5 categories) with a softmax activation.\n",
    "\n",
    "\n",
    "- You should also compile your model using an appropiate loss function and optimizer.\n",
    "\n",
    "\n",
    "- You can use any architecture you want but keep in mind that this problem doesn't need many layers to be solved successfully. You don't need any layers beside Embedding, [GlobalAveragePooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) and Dense layers but feel free to try out different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "777e43f4",
   "metadata": {
    "id": "777e43f4"
   },
   "outputs": [],
   "source": [
    "# Actual Model.  Can you replace this part? With what?\n",
    "\n",
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \n",
    "    #tf.random.set_seed(123)\n",
    "    \n",
    "    ### START CODE HERE\n",
    "\n",
    "    lstm1_dim = 400\n",
    "    lstm2_dim = 200\n",
    "    lstm3_dim = 100\n",
    "    dense1_dim = 400\n",
    "    dense2_dim = 100\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words+1, embedding_dim, input_length=maxlen),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm3_dim)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(dense1_dim, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(300, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(dense2_dim, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "528697fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "528697fa",
    "outputId": "ce446884-c2d4-45d6-8923-55c958eb4944"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 6000\n",
    "EMBEDDING_DIM = 200\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(train_padded_seq, train_label_seq, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_padded_seq, val_label_seq), callbacks=[callback], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c432bfb",
   "metadata": {
    "id": "7c432bfb"
   },
   "source": [
    "Once training has finished you can run the following cell to check the training and validation accuracy achieved at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_rW5Xy__gC1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "i_rW5Xy__gC1",
    "outputId": "4b49366a-74be-42d1-8141-4110cf10cad4"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, f'val_{metric}'])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac0880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "39ac0880",
    "outputId": "50cdfae2-f88e-413e-d982-0fd699547201"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c69b1a",
   "metadata": {
    "id": "41c69b1a"
   },
   "source": [
    "## Visualizing 3D Vectors\n",
    "\n",
    "We can visualize the vectors associated with each word in the training set in a 3D space.\n",
    "\n",
    "For this run the following cells and visit [Tensorflow's Embedding Projector](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d245f",
   "metadata": {
    "id": "b95d245f"
   },
   "outputs": [],
   "source": [
    "# Reverse word index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Save the embedding layer\n",
    "e = model.layers[0]\n",
    "\n",
    "# Save the weights of the embedding layer\n",
    "weights = e.get_weights()[0]\n",
    "print(f\"Weights of embedding layer have shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb987604",
   "metadata": {
    "id": "eb987604"
   },
   "source": [
    "Now run the following cell to generate the `vecs.tsv` and `meta.tsv` files that you will upload to the embedding projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450751a",
   "metadata": {
    "id": "7450751a"
   },
   "outputs": [],
   "source": [
    "# Generate files for embedding visualization\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, NUM_WORDS):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "arabizi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
