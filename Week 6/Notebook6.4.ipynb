{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI614: Data Science & its Applications\n",
    "\n",
    "*Notebook 6.4: Track this disease*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI614/blob/main/Week%206/Notebook6.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import KMeans\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mean = 0\n",
    "variance = 2\n",
    "\n",
    "# Modeling dart coordinates using two Normal distributions\n",
    "x = np.random.normal(mean, variance ** 0.5)\n",
    "y = np.random.normal(mean, variance ** 0.5)\n",
    "print(f\"The x coordinate of a randomly thrown dart is {x:.2f}\")\n",
    "print(f\"The y coordinate of a randomly thrown dart is {y:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulating randomly thrown darts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "bulls_eye1 = [0, 0]\n",
    "bulls_eye2 = [6, 0]\n",
    "bulls_eyes = [bulls_eye1, bulls_eye2]\n",
    "x_coordinates, y_coordinates = [], []\n",
    "for bulls_eye in bulls_eyes:\n",
    "    for _ in range(5000):\n",
    "        x = np.random.normal(bulls_eye[0], variance ** 0.5)\n",
    "        y = np.random.normal(bulls_eye[1], variance ** 0.5)\n",
    "        x_coordinates.append(x)\n",
    "        y_coordinates.append(y)\n",
    "        \n",
    "plt.scatter(x_coordinates, y_coordinates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assigning darts to the nearest bull’s-eye**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "def nearest_bulls_eye(dart):\n",
    "    distances = [euclidean(dart, bulls_e) for bulls_e in bulls_eyes]\n",
    "    return np.argmin(distances)\n",
    "\n",
    "darts = [[0,1], [6, 1]]\n",
    "for dart in darts:\n",
    "    index = nearest_bulls_eye(dart)\n",
    "    print(f\"The dart at position {dart} is closest to bulls-eye {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the `nearest_bulls_eye` function to all our computed color coordinates. Afterwards, each dart-point will be plotted using one of two colors, in order to distinguish between the two bull's-eye assignments.\n",
    "\n",
    "**Coloring darts based on nearest bull’s-eye**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_by_cluster(darts):\n",
    "    nearest_bulls_eyes = [nearest_bulls_eye(dart) for dart in darts]\n",
    "    for bs_index in range(len(bulls_eyes)):\n",
    "        selected_darts = [darts[i] for i in range(len(darts))\n",
    "                          if bs_index == nearest_bulls_eyes[i]]\n",
    "        x_coordinates, y_coordinates = np.array(selected_darts).T\n",
    "        plt.scatter(x_coordinates, y_coordinates, \n",
    "                    color=['g', 'k'][bs_index])\n",
    "    plt.show()\n",
    "\n",
    "darts = [[x_coordinates[i], y_coordinates[i]]  \n",
    "         for i in range(len(x_coordinates))]\n",
    "color_by_cluster(darts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colored darts sensibly split into two even clusters. How would we identify such clusters if no central coordinates were provided? Well, one primitive strategy is to simply guess the location of the bull's-eyes.\n",
    "\n",
    "**Assigning darts to randomly chosen centers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulls_eyes = np.array(darts[:2])\n",
    "color_by_cluster(darts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster B on the right seems to be stretching way too far to the left.  Lets remedy our error. We 'll compute the mean coordinates of all the points within the stretched right clustered group, and afterwards utilize these coordinates to adjust our estimation of the group's center. We will also reset the left-most cluster's center to its mean prior to re-running our centrality-based clustering.\n",
    "\n",
    "**Assigning darts to centers based on mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bulls_eyes(darts):\n",
    "    updated_bulls_eyes = []\n",
    "    nearest_bulls_eyes = [nearest_bulls_eye(dart) for dart in darts]\n",
    "    for bs_index in range(len(bulls_eyes)):\n",
    "        selected_darts = [darts[i] for i in range(len(darts))\n",
    "                          if bs_index == nearest_bulls_eyes[i]]\n",
    "        x_coordinates, y_coordinates = np.array(selected_darts).T\n",
    "        mean_center = [np.mean(x_coordinates), np.mean(y_coordinates)]\n",
    "        updated_bulls_eyes.append(mean_center)\n",
    "        \n",
    "    return updated_bulls_eyes\n",
    "\n",
    "bulls_eyes = update_bulls_eyes(darts)\n",
    "color_by_cluster(darts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster's centers still appear a little off. Lets remedy the results by repeating the mean-based centrality adjustment over 10 additional iterations.\n",
    "\n",
    "**Adjusting bull’s-eye positions over 10 iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    bulls_eyes = update_bulls_eyes(darts)\n",
    "    \n",
    "color_by_cluster(darts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two sets of darts have been perfectly clustered. We have essentially replicated the  **K-means** clustering algorithm, which organizes data using centrality.\n",
    "\n",
    "## K-Means: A Clustering Algorithm for Grouping Data into K Central Groups\n",
    "\n",
    "### K-means Clustering Using Scikit-Learn\n",
    "\n",
    "A speedy implementation of the K-means algorithm is available through the external Scikit-Learn library. Lets import Scikit-learn's `KMeans` clustering class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `KMeans` class to cluster our `darts` data.\n",
    "\n",
    "**K-means clustering using Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = KMeans(n_clusters=2)\n",
    "assigned_bulls_eyes = cluster_model.fit_predict(darts)\n",
    "\n",
    "print(\"Bull's-eye assignments:\")\n",
    "print(assigned_bulls_eyes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly color our darts based on their clustering assignments, in order to confirm that the assignments makes sense.\n",
    "\n",
    "**Plotting K-means cluster assignments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs_index in range(len(bulls_eyes)):\n",
    "    selected_darts = [darts[i] for i in range(len(darts))\n",
    "                      if bs_index == assigned_bulls_eyes[i]]\n",
    "    x_coordinates, y_coordinates = np.array(selected_darts).T\n",
    "    plt.scatter(x_coordinates, y_coordinates, \n",
    "                color=['g', 'k'][bs_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our clustering model has located the centroids in the data. Now, we can reuse these centroids to analyze new data-points that the model has not seen before.\n",
    "\n",
    "**Using `cluster_model` to cluster new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_darts = [[500, 500], [-500, -500]]\n",
    "new_bulls_eye_assignments = cluster_model.predict(new_darts)\n",
    "for i, dart in enumerate(new_darts):\n",
    "    bulls_eye_index = new_bulls_eye_assignments[i]\n",
    "    print(f\"Dart at {dart} is closest to bull's-eye {bulls_eye_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Optimal K Using the Elbow Method\n",
    "\n",
    "We estimate an appropriate value for K using a technique known as the **Elbow method**. The Elbow method depends on a calculated value called **inertia**, which is the sum of the squared distances between each point and its closest K-means center. We'll run the technique by plotting the inertia of our dartboard dataset over a large range of K values.\n",
    "\n",
    "**Plotting the K-means inertia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 10)\n",
    "inertia_values = [KMeans(k).fit(darts).inertia_\n",
    "                  for k in k_values]\n",
    "\n",
    "plt.plot(k_values, inertia_values)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated plot resembles an arm bent at the elbow. The elbow points directly to a K of 2. What will happen if the number of centers is increased? We can find out by adding an additional bull's-eye to our dart-throwing simulation.\n",
    "\n",
    "**Plotting inertia for a 3-dartboard simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bulls_eye = [12, 0]\n",
    "for _ in range(5000):\n",
    "    x = np.random.normal(new_bulls_eye[0], variance ** 0.5)\n",
    "    y = np.random.normal(new_bulls_eye[1], variance ** 0.5)\n",
    "    darts.append([x, y])\n",
    "\n",
    "inertia_values = [KMeans(k).fit(darts).inertia_\n",
    "                  for k in k_values]\n",
    "\n",
    "plt.plot(k_values, inertia_values)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Density to Discover Clusters\n",
    "\n",
    "Suppose that an astronomer discovers a new planet at the far-flung edges of the solar system. The planet, much like our Saturn, has multiple rings spinning in constant orbit around its center. Each ring is formed from thousands of rocks. We'll model these rocks as individual points, defined by x and y coordinates.\n",
    "\n",
    "**Simulating rings around a planet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "x_coordinates = []\n",
    "y_coordinates = []\n",
    "for factor in [.3, .6, 0.99]:\n",
    "    rock_ring, _ = make_circles(n_samples=800, factor=factor,\n",
    "                                noise=.03, random_state=1)\n",
    "    for rock in rock_ring:\n",
    "        x_coordinates.append(rock[0])\n",
    "        y_coordinates.append(rock[1])\n",
    "\n",
    "plt.scatter(x_coordinates, y_coordinates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three ring-groups are clearly present in the plot. Lets search for these three clusters using K-means.\n",
    "\n",
    "**Using K-means to cluster rings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocks = [[x_coordinates[i], y_coordinates[i]]  \n",
    "          for i in range(len(x_coordinates))]\n",
    "rock_clusters = KMeans(3).fit_predict(rocks)\n",
    "\n",
    "colors = [['g', 'y', 'k'][cluster] for cluster in  rock_clusters]\n",
    "plt.scatter(x_coordinates, y_coordinates, color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is an utter failure! We need to design an algorithm that will cluster data within dense regions of space. One simple definition of density is as follows; a point is in a dense region only if it's located within a distance `epsilon` of `min_points` other points.  Below, we'll set `epsilon` to 0.1 and `min_points` to 10.\n",
    "\n",
    "**Specifying density parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "min_points = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the density of the first rock in our `rocks` list. We'll begin by searching for all the other rocks that are within `epsilon` units of `rocks[0]`.\n",
    "\n",
    "**Finding the neighbors of `rocks[0]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices = [i for i, rock in enumerate(rocks[1:])\n",
    "                    if euclidean(rocks[0], rock) <= epsilon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll compare the number of neighbors to `min_points`, in order to determine if `rocks[0]` lies in a dense region of space.\n",
    "\n",
    "**Checking the density of `rocks[0]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors = len(neighbor_indices)\n",
    "print(f\"The rock at index 0 has {num_neighbors} neighbors.\")\n",
    "\n",
    "if num_neighbors >= min_points:\n",
    "    print(\"It lies in a dense region.\")\n",
    "else:\n",
    "    print(\"It does not lie in a dense region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rock at index 0 lies in a dense region of space. We can combine `rocks[0]` and its neighbors into a single dense cluster.\n",
    "\n",
    "**Creating a dense cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_region_indices = [0] + neighbor_indices\n",
    "dense_region_cluster = [rocks[i] for i in dense_region_indices]\n",
    "dense_cluster_size = len(dense_region_cluster)\n",
    "print(f\"We found a dense cluster containing {dense_cluster_size} rocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rock and index 0 and its neighbors form a single 41-element dense cluster. What about the neighbors of the neighbors? By analyzing additional neighboring points, we expand the size of `dense_region_cluster`.\n",
    "\n",
    "**Expanding a dense cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_region_indices = set(dense_region_indices)\n",
    "for index in neighbor_indices:\n",
    "    point = rocks[index]\n",
    "    neighbors_of_neighbors = [i for i, rock in enumerate(rocks)\n",
    "                              if euclidean(point, rock) <= epsilon]\n",
    "    if len(neighbors_of_neighbors) >= min_points:\n",
    "        dense_region_indices.update(neighbors_of_neighbors)\n",
    "            \n",
    "dense_region_cluster = [rocks[i] for i in dense_region_indices]\n",
    "dense_cluster_size = len(dense_region_cluster)\n",
    "print(f\"We expanded our cluster to include {dense_cluster_size} rocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand our cluster even further by analyzing the density of newly encountered neighbors. Iteratively repeating our analysis will increase the breadth of our cluster boundary. This precedure is known as **DBSCAN**. The DBSCAN algorithm organizes data based on its density distribution.\n",
    "\n",
    "## DBSCAN: A Clustering Algorithm for Grouping Data Based on Spatial Density\n",
    "\n",
    "Scikit-Learn makes DBSCAN available for use. We simply need to import the `DBSCAN` class from `sklearn.cluster`. Afterwards, we can initialize the class by assigning `epsilon` and `min_points` using the `eps` and `min_samples` parameters. Lets utilize `DBSCAN` to cluster our 3 rings.\n",
    "\n",
    "**Using `DBSCAN` to cluster rings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "cluster_model = DBSCAN(eps=epsilon, min_samples=min_points)\n",
    "rock_clusters = cluster_model.fit_predict(rocks)\n",
    "colors = [['g', 'y', 'k'][cluster] for cluster in rock_clusters]\n",
    "plt.scatter(x_coordinates, y_coordinates, color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN has successfully identified the 3 rock rings. The algorithm succeeded where K-means had failed.\n",
    "\n",
    "### Comparing DBSCAN and K-means\n",
    "\n",
    "DBSCAN can filter random outliers located in sparse regions of space. For example, if we add an outlier located beyond the boundary of the rings, then DBSCAN will assign it a cluster id of -1. The negative value indicates that the outlier cannot be clustered with the rest of the dataset.\n",
    "\n",
    "**Finding outliers using DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_data = rocks + [[1000, -1000]]\n",
    "clusters = DBSCAN(eps=epsilon, \n",
    "                  min_samples=min_points).fit_predict(noisy_data)\n",
    "assert clusters[-1] == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one other advantage to the DBSCAN technique that is missing from K-means. DBSCAN does not depend on Euclidean distance.\n",
    "\n",
    "### Clustering Based on Non-Euclidean Distance\n",
    "\n",
    "Suppose we are visiting Manhattan. We wish to know the walking distance from the Empire State Building to Columbus Circle. The Empire State Building is located at the intersection of 34th street and 5th avenue. Meanwhile, Columbus Circle is located at the intersection of 57th street and 8th avenue. Our route requires us to walk 26 blocks total. Manhattan's average block-length is 0.17 miles. Lets compute that walking distance directly using a generalized `manhattan_distance` function.\n",
    "\n",
    "**Computing the Manhattan distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(point_a, point_b):\n",
    "    num_blocks = np.sum(np.absolute(point_a - point_b))\n",
    "    return 0.17 * num_blocks\n",
    "\n",
    "x = np.array([34, 5])\n",
    "y = np.array([57, 8])\n",
    "distance = manhattan_distance(x, y)\n",
    "\n",
    "print(f\"Manhattan distance is {distance} miles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we wish to cluster more than two Manhattan locations, using DBSCAN. We will pass `metric= manhattan_distance` into the initialization method. Consequently, the clustering distance will correctly reflect the grid-based constraints within the City.\n",
    "\n",
    "**Clustering using Manhattan distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [[35, 5], [33, 6], [37, 4], [40, 7], [45, 5]]\n",
    "clusters = DBSCAN(eps=1, min_samples=3,\n",
    "                  metric=manhattan_distance).fit_predict(points)\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    point = points[i]\n",
    "    if cluster == -1:\n",
    "        print(f\"Point at index {i} is an outlier\")\n",
    "        plt.scatter(point[0], point[1], marker='x', color='k')\n",
    "    else:\n",
    "        print(f\"Point at index {i} is in cluster {cluster}\")\n",
    "        plt.scatter(point[0], point[1], color='g')\n",
    "\n",
    "plt.grid(True, which='both', alpha=0.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike K-means, the DBSCAN algorithm does not require our distance function to be linearly divisible. Thus, we can easily run DBSCAN clustering using a ridiculous distance metric.\n",
    "\n",
    "**Clustering using a ridiculous measure of distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridiculous_measure(point_a, point_b):\n",
    "    is_negative_a = np.array(point_a) < 0\n",
    "    is_negative_b = np.array(point_b) < 0\n",
    "    if is_negative_a.all() and is_negative_b.all():\n",
    "        return 0\n",
    "    elif is_negative_a.any() or is_negative_b.any():\n",
    "        return 10\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "points = [[-1, -1], [-10, -10], [-1000, -13435], [3,5], [5,-7]]\n",
    "                   \n",
    "clusters = DBSCAN(eps=.1, min_samples=2,\n",
    "                  metric=ridiculous_measure).fit_predict(points)\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    point = points[i]\n",
    "    if cluster == -1:\n",
    "        print(f\"{point} is an outlier\")\n",
    "    else:\n",
    "        print(f\"{point} falls in cluster {cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Clusters Using Pandas\n",
    "\n",
    "We can more intuitively analyze clustered rocks by combining the coordinates and the clusters together in a single Pandas table.\n",
    "\n",
    "**Storing clustered coordinates in a table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x_coordinates, y_coordinates = np.array(rocks).T\n",
    "df = pd.DataFrame({'X': x_coordinates, 'Y': y_coordinates,\n",
    "                   'Cluster': rock_clusters})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Pandas table lets us easily access the rocks in any cluster. Lets plot those rocks that fall into cluster zero.\n",
    "\n",
    "**Plotting a single cluster using Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df[df.Cluster == 0]\n",
    "plt.scatter(df_cluster.X, df_cluster.Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows us to obtain a table containing elements from any single cluster. Alternatively, we might want to obtain multiple tables, where each table maps to a cluster id. In Pandas, this can easily be done by calling `df.groupby('Cluster')`.\n",
    "\n",
    "**Iterating over clusters using Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id, df_cluster in df.groupby('Cluster'):\n",
    "    if cluster_id == 0:\n",
    "        print(f\"Skipping over cluster {cluster_id}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Plotting cluster {cluster_id}\")\n",
    "    plt.scatter(df_cluster.X, df_cluster.Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Geographic Location Visualization and Analysis\n",
    "\n",
    "## The Great-Circle Distance: A Metric for Computing Distances Between Two Global Points\n",
    "\n",
    "The direct path between two points along the surface of a sphere is called the **great-circle distance**. That distance  depends on a series of well-known trigonometric operations.\n",
    "\n",
    "**Defining a great-circle distance function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, sin, asin\n",
    "\n",
    "def great_circle_distance(x1, y1, x2, y2):\n",
    "    delta_x, delta_y = x2 - x1, y2 - y1\n",
    "    haversin = sin(delta_x / 2) ** 2 + np.product([cos(x1), cos(x2), \n",
    "                                                   sin(delta_y / 2) ** 2])\n",
    "    return 2 * asin(haversin ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the great-circle distance between two points that lie 180 degrees apart, relative to both the x-axis and the y-axis.\n",
    "\n",
    "**Computing the great-circle distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "distance = great_circle_distance(0, 0, 0, pi)\n",
    "print(f\"The distance equals {distance} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points are exactly π units apart, half the distance required to circumnavigate a unit-circle. This is akin to traveling between the North and South Poles of any planet. We'll confirm by analyzing the latitudes and longitudes of Earth's North Pole and South Pole. Lets begin by recording the known coordinates of each pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_north, longitude_north = (90.0, 0)\n",
    "latitude_south, longitude_south = (-90.0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitudes and longitudes measure spherical coordinates in degrees, not radians. We'll thus convert to radians from degrees using the `np.radians` function. Afterwards, we'll input the radian results into `great_circle_distance`.\n",
    "\n",
    "**Computing the great-circle distance between poles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_radians =  np.radians([latitude_north, longitude_north, \n",
    "                          latitude_south, longitude_south])\n",
    "distance = great_circle_distance(*to_radians.tolist())\n",
    "print(f\"The unit-circle distance between poles equals {distance} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the distance between poles on a unit-sphere is π . Now, let's measure the distance between two poles here on Earth. The radius of the Earth is not 1 hypothetical unit, but rather 3956 actual miles.\n",
    "\n",
    "**Computing the travel distance between Earth’s poles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earth_distance = 3956 * distance\n",
    "print(f\"The distance between poles equals {earth_distance} miles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a general `travel_distance` function to calculate the travel mileage between any two terrestrial points.\n",
    "\n",
    "**Defining a travel distance function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def travel_distance(lat1, lon1, lat2, lon2):\n",
    "    to_radians =  np.radians([lat1, lon1, lat2, lon2])\n",
    "    return 3956 * great_circle_distance(*to_radians.tolist())\n",
    "\n",
    "assert travel_distance(90, 0, -90, 0) == earth_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Maps Using Cartopy\n",
    "\n",
    "Cartopy is a Matplotlib extension for generating maps in Python. Lets install and import the Cartopy library.\n",
    "\n",
    "**Importing the Cartopy library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A geographic map is a 2D representation of a 3D surface on a globe. Flattening the spherical globe is carried out using a process called **projection**. The simplest projection involves superimposing the globe on an unrolled cylinder, and is called the **Plate Carrée projection**. We'll utilize this standard projection in our plots by importing `PateCarree` from `cartopy.csr`.\n",
    "\n",
    "**Importing the Plate Carrée projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartopy.crs import PlateCarree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PlateCarree` class can be used in conjunction with Matploblib in order to visualize the Earth. For instance, running `plt.axes(projection=PlateCarree()).coastlines()` will plot the outlines of the Earth's seven continents.\n",
    "\n",
    "**Visualizing the Earth using Cartopy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes(projection=PlateCarree()).coastlines()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plotted map is a bit small. We can increase map-size using Matplotlib's `plt.figure` function.\n",
    "\n",
    "**Visualizing a larger map of the Earth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.axes(projection=PlateCarree()).coastlines()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our map looks sparse and uninviting. We can improve the quality by calling `plt.axes(projection=PlateCarree()).stock_img()`. The method-call will color the map using topographic information.\n",
    "\n",
    "**Coloring a map of the Earth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.axes(projection=PlateCarree()).stock_img()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "National boundaries are currently missing from the plot. Let's add them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.stock_img()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that `ax.stock_img()` relies on a saved stock image of the Earth to color the map. Alternatively, we can color the oceans and the continents using the `ax.add_features` method. The method displays special Cartopy features, which are stored in the `cartopy.feature` module.\n",
    "\n",
    "**Adding colors with the `features` module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.OCEAN)\n",
    "ax.add_feature(cartopy.feature.LAND)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, national borders are currently missing from the plot.\n",
    "Cartopy treats these borders as a feature in the `features` module. We can incorporate country borders by calling `ax.add_feature(cartopy.feature.BORDERS)`.\n",
    "\n",
    "**Adding national borders to the plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.BORDERS)\n",
    "ax.add_feature(cartopy.feature.OCEAN)\n",
    "ax.add_feature(cartopy.feature.LAND)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given a list of locations defined by pairs of latitudes and longitudes. We can plot these locations on our map by separating the latitudes from the longitudes and then passing the results into `ax.scatter`.\n",
    "\n",
    "**Plotting coordinates on a map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "coordinates = [(39.9526, -75.1652), (37.7749, -122.4194),\n",
    "               (40.4406, -79.9959), (38.6807, -108.9769),\n",
    "               (37.8716, -112.2727), (40.7831, -73.9712)]\n",
    "\n",
    "latitudes, longitudes = np.array(coordinates).T\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "ax.scatter(longitudes, latitudes)\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotted points all fall within the borders of North America. We can thusly simplify the map by zooming-in on that continent. However, first we’ll need to adjust the **map extent**, which is the geographic area shown on a map.\n",
    "\n",
    "**Plotting North American Coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "\n",
    "north_america_extent = (-145, -50, 0, 90)\n",
    "ax.set_extent(north_america_extent)\n",
    "ax.scatter(longitudes, latitudes, color='r')\n",
    "\n",
    "def add_map_features():\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cartopy.feature.BORDERS)\n",
    "    ax.add_feature(cartopy.feature.OCEAN)\n",
    "    ax.add_feature(cartopy.feature.LAND)\n",
    "\n",
    "add_map_features()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully zoomed in on North America. Now, we'll zoom in further, onto the United States. Unfortunately, the Plate Carrée projection will prove insufficient for this purpose. That technique distorts the map if we zoom-in too close to any country. Instead, we will on rely on the **Lambert Conformal Conic projection**.\n",
    "\n",
    "**Plotting USA Coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartopy.crs import LambertConformal\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=LambertConformal())\n",
    "us_extent = (-120, -75, 20, 50)\n",
    "ax.set_extent(us_extent)\n",
    "\n",
    "ax.scatter(longitudes, latitudes, color='r', \n",
    "           transform=PlateCarree(),\n",
    "           s=100)\n",
    "\n",
    "add_map_features()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our map of the United States is looking a little sparse. Lets add state borders to the map by calling ``ax.add_feature(cartopy.feature.STATES)`.\n",
    "\n",
    "**Plotting a US map with state border inclusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=LambertConformal())\n",
    "ax.set_extent(us_extent)\n",
    "\n",
    "ax.scatter(longitudes, latitudes, color='r', \n",
    "           transform=PlateCarree(),\n",
    "           s=100)\n",
    "\n",
    "ax.add_feature(cartopy.feature.STATES)\n",
    "add_map_features()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basemap allows us to plot any location on a map. All we need is the location’s latitude and longitude. Thus, we need a mapping between location names and their geographic properties. That mapping is provided by the GeoNamesCache location-tracking library.\n",
    "\n",
    "## Location Tracking Using GeoNamesCache\n",
    "\n",
    "GeoNamesCache is designed to efficiently retrieve data pertaining to continents, countries, and cities, as well US counties and US states. Lets install the library and explore its usage in more detail. We'll begin by initializing a `GeonamesCache` location-tracking object.\n",
    "\n",
    "**Initializing a `GeonamesCache` object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geonamescache import GeonamesCache\n",
    "gc = GeonamesCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use our `gc` object to explore the 7 continents. We'll run `gc.get_continents()` in order to retrieve a dictionary of continent-related information.\n",
    "\n",
    "**Fetching all seven continents from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = gc.get_continents()\n",
    "print(continents.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary keys represent shorthand encoding of continent names, in which __Africa__ is transformed into `'AF'`, and __North America__ is transformed into `'NA'`. Lets check the values mapped to every key by passing in the code for __North America__.\n",
    "\n",
    "**Fetching North America from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_america = continents['NA']\n",
    "print(north_america.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the `north_america` data elements represent the various naming schemes for the North American continent. Such information is not very useful.\n",
    "\n",
    "**Printing North America’s naming schemas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_key in ['name', 'asciiName', 'toponymName']:\n",
    "    print(north_america[name_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `'lat'` and the `'lng'` keys map to the latitude and longitude of the central-most location in the continent. We can utilize these coordinates to plot a map projection centered at the heart of North America.\n",
    "\n",
    "**Mapping North America’s central coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = float(north_america['lat'])\n",
    "longitude = float(north_america['lng'])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "ax.set_extent(north_america_extent)\n",
    "ax.scatter([longitude], [latitude], s=200)\n",
    "add_map_features()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Country Information\n",
    "We can analyze countries using the `get_countries` method. It returns a dictionary whose 2-character keys encode the names of 252 different countries. Accessing `gc.get_countries()['US']` will return a dictionary containing useful USA statistics. Lets output all the non-city information pertaining to the United States.\n",
    "\n",
    "**Fetching US data from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = gc.get_countries()\n",
    "num_countries = len(countries)\n",
    "print(f\"GeonamesCache holds data for {num_countries} countries.\")\n",
    "\n",
    "us_data = countries['US']\n",
    "print(\"The following data pertains to the United States:\")\n",
    "print(us_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is valuable information within each country's `'neighbours'` element. It maps to a comma-delimited string\n",
    "of country codes that signify all neighboring territories. We can obtain more details about each neighbor by splitting the string and passing the codes into the `'countries'` dictionary.\n",
    "\n",
    "**Fetching neighboring countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_neighbors = us_data['neighbours']\n",
    "for neighbor_code in us_neighbors.split(','):\n",
    "    print(countries[neighbor_code]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query all countries by name using the `get_countries_by_names` method. This method returns a dictionary whose elements are country names rather than codes.\n",
    "\n",
    "**Fetching countries by name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gc.get_countries_by_names()['United States']\n",
    "assert result == countries['US']\n",
    "countries['US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing City Information\n",
    "\n",
    "The `get_cities` method returns a dictionary whose keys are unique ids mapping back to city data.\n",
    "\n",
    "**Fetching cities from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = gc.get_cities()\n",
    "num_cities = len(cities)\n",
    "print(f\"GeoNamesCache holds data for {num_cities} total cities\")\n",
    "city_id = list(cities.keys())[0]\n",
    "print(cities[city_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for each city contains the reference code for the country where that city is located. By utilizing the country code, we can create a new mapping between a country and all its territorial cities.\n",
    "\n",
    "**Fetching US cities from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities = [city for city in cities.values() \n",
    "             if city['countrycode'] == 'US']\n",
    "num_us_cities = len(us_cities)\n",
    "print(f\"GeoNamesCache holds data for {num_us_cities} US cities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the average US latitude and longitude. This average will approximate the central coordinates of the United States.\n",
    "\n",
    "**Approximating US central coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_lat = np.mean([city['latitude'] \n",
    "                      for city in us_cities])\n",
    "center_lon = np.mean([city['longitude'] \n",
    "                       for city in us_cities])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=LambertConformal())\n",
    "ax.set_extent(us_extent)\n",
    "ax.scatter([center_lon], [center_lat], transform=PlateCarree(), s=200)\n",
    "ax.add_feature(cartopy.feature.STATES)\n",
    "add_map_features()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_cities` method is suitable for iterating over city information, but not for querying cities by name. To run a name-based city search, we must rely on `get_cities_by_name`. \n",
    "\n",
    "**Fetching cities by name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_cities_list = gc.get_cities_by_name('Philadelphia')\n",
    "print(matched_cities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_cities_by_name` method may return more than one city, because city-names are not always unique. For example, GeoNamesCache contains 6 different instances of the city __San Francisco__, spanning across 5 different countries.\n",
    "\n",
    "**Fetching multiple cities with a shared name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_cities_list = gc.get_cities_by_name('San Francisco')\n",
    "\n",
    "for i, san_francisco in enumerate(matched_cities_list):\n",
    "    city_info = list(san_francisco.values())[0]\n",
    "    country_code = city_info['countrycode']\n",
    "    country = countries[country_code]['name']\n",
    "    print(f\"The San Francisco at index {i} is located in {country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its common for multiple cities to share the same name. Choosing among such cities is quite difficult. Usually, the safest guess is the city with the largest population. \n",
    "\n",
    "**Mapping the most populous San Francisco**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sf = max(gc.get_cities_by_name('San Francisco'), \n",
    "              key=lambda x: list(x.values())[0]['population'])\n",
    "sf_data = list(best_sf.values())[0]\n",
    "sf_lat = sf_data['latitude']\n",
    "sf_lon = sf_data['longitude']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=LambertConformal())\n",
    "ax.set_extent(us_extent)\n",
    "ax.scatter(sf_lon, sf_lat, transform=PlateCarree(), s=200)\n",
    "add_map_features()\n",
    "ax.text(sf_lon + 1, sf_lat, ' San Francisco', fontsize=16, transform=PlateCarree())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the GeoNamesCache Library\n",
    "\n",
    "The `get_cities_by_name` method maps only one version of a city's name to its geographic data. This poses a problem for cities like __New York__, which carry more than one commonly referenced name.\n",
    "\n",
    "**Fetching New York City from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ny_name in ['New York', 'New York City']:\n",
    "    if not gc.get_cities_by_name(ny_name):\n",
    "        print(f\"'{ny_name}' is not present in GeoNamesCache database.\")\n",
    "    else:\n",
    "        print(f\"'{ny_name}' is present in GeoNamesCache database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limits of single references become particularly obvious when we examine diacritics in city names. Diacritics are accent marks that designate the proper pronunciation of foreign-sounding words. \n",
    "\n",
    "**Fetching accented cities from GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gc.get_cities_by_name(u'Cañon City'))\n",
    "print(gc.get_cities_by_name(u'Hagåtña'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the cities stored in GeoNamesCache contain diacritics in their name? We can find out using the `unidecode` function from the external Unidecode library.\n",
    "\n",
    "**Counting all accented cities in GeoNamesCache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "accented_names = [city['name'] for city in gc.get_cities().values()\n",
    "                  if city['name'] != unidecode(city['name'])]\n",
    "num_accented_cities = len(accented_names)\n",
    "\n",
    "print(f\"An example accented city name is '{accented_names[0]}'\")\n",
    "print(f\"{num_accented_cities} cities have accented names\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now match the stripped dictionary keys against all inputted text by passing the accented dictionary values into GeoNamesCache, whenever a key-match is found.\n",
    "\n",
    "**Finding accent-free city-names in text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_names = {unidecode(name): name \n",
    "                     for name in accented_names}\n",
    "print(gc.get_cities_by_name(alternative_names['Hagatna']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Location Names in Text\n",
    "\n",
    "In Python, we can easily determine if one string is a substring of another, or if the start of a string contains some predefined text.\n",
    "\n",
    "**Basic string matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u'This sentence matches Hagatna'\n",
    "for key, value in alternative_names.items():\n",
    "    if key in text:\n",
    "        print(gc.get_cities_by_name(value))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more complex analyses, Python's basic string syntax can be quite limiting. For example, Python's string methods can't directly distinguish between sub-characters in a string and sub-phrases in a sentence.\n",
    "\n",
    "**Basic substring matching errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'in a' in 'sin apple'\n",
    "assert 'in a' in 'win attached'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcame these limitations, we must rely on Python's built-in regular expression processing library, `re`. A **regular expression** (or **regex** for short) is a string-encoded pattern that can be compared against some text. Most regex-matching in Python can be executed with the `re.search` function. \n",
    "\n",
    "**String matching using regexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = 'Boston'\n",
    "random_text = 'Clown Patty'\n",
    "match = re.search(regex, random_text)\n",
    "assert match is None\n",
    "\n",
    "matchable_text = 'Boston Marathon'\n",
    "match = re.search(regex, matchable_text)\n",
    "assert match is not None\n",
    "start, end = match.start(), match.end()\n",
    "matched_string = matchable_text[start: end]\n",
    "assert matched_string == 'Boston'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case-insensitive string matching is a breeze with `re.search`. We simply pass `re.IGNORECASE` as an added `flags` parameter.\n",
    "\n",
    "**Case-insensitive matching using regexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ['BOSTON', 'boston', 'BoSTOn']:\n",
    "    assert re.search(regex, text, flags=re.IGNORECASE) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, regexes permit us to match exact words, and not just substrings, using word boundary detection.\n",
    "\n",
    "**Word boundary matching using regexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regex in ['\\\\bin a\\\\b', r'\\bin a\\b']:\n",
    "    for text in ['sin apple', 'win attached']:\n",
    "        assert re.search(regex, text) is None\n",
    "        \n",
    "    text = 'Match in a string'\n",
    "    assert re.search(regex, text) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us carry out a more complicated match. We'll match against the sentence `f'I visited {city} yesterday`, where `{city}` represents one of 3 possible locations; `'Boston'`, `'Philadelphia'`, or `'San Francisco'`.\n",
    "\n",
    "**Multi-city matching using regexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'I visited \\b(Boston|Philadelphia|San Francisco)\\b yesterday.'\n",
    "assert re.search(regex, 'I visited Chicago yesterday.') is None\n",
    "\n",
    "cities = ['Boston', 'Philadelphia', 'San Francisco']\n",
    "for city in cities:\n",
    "    assert re.search(regex, f'I visited {city} yesterday.') is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to match a regex against 100 strings. For every match, `re.search`  will transform the regex into Python `PatternObject.` Each such transformation is computationally costly. We're better off executing the transformation only once using `re.compile`.\n",
    "\n",
    "**String matching using compiled regexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_re = re.compile(regex)\n",
    "text = 'I visited Boston yesterday.'\n",
    "for i in range(1000):\n",
    "    assert compiled_re.search(text) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Case Study: Track this Disease by extracting Locations from Headline Data\n",
    "\n",
    "We'll begin by loading the headline data.\n",
    "\n",
    "**Loading headline data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_file = open('headlines.txt','r')\n",
    "headlines = [line.strip() \n",
    "             for line in headline_file.readlines()]\n",
    "num_headlines = len(headlines)\n",
    "print(f\"{num_headlines} headlines have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a mechanism for extracting city and country names from the headline text. One naïve solution is to match the locations in GeoNamesCache against each and every headline.  However, for more optimal matching, we should transform each location name into a case-independent and accent-independent regular expression. Lets execute these transformations using a custom `name_to_regex` function. \n",
    "\n",
    "**Converting names to regexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_regex(name):\n",
    "    decoded_name = unidecode(name)\n",
    "    if name != decoded_name:\n",
    "        regex = fr'\\b({name}|{decoded_name})\\b'\n",
    "    else:\n",
    "        regex = fr'\\b{name}\\b'\n",
    "    return re.compile(regex, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `name_to_regex`, we can create create a mapping between regular expressions and the original names in GeoNamesCache.\n",
    "\n",
    "**Mapping names to regexe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [country['name'] \n",
    "             for country in gc.get_countries().values()]\n",
    "country_to_name = {name_to_regex(name): name \n",
    "                   for name in countries}\n",
    "                   \n",
    "cities = [city['name'] for city in gc.get_cities().values()]\n",
    "city_to_name = {name_to_regex(name): name for name in cities}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll use our mappings to define a function that will look for location names in text\n",
    "\n",
    "**Finding locations in text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_in_text(text, dictionary):\n",
    "    for regex, name in sorted(dictionary.items(), \n",
    "                              key=lambda x: x[1]):\n",
    "        if regex.search(text):\n",
    "            return name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll utilize `get_names_in_text` to discover the cities and countries that are mentioned in the `headlines` list. Afterwards, we'll store the results in a Pandas table for easier analysis.\n",
    "\n",
    "**Finding locations in headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "matched_countries = [get_name_in_text(headline, country_to_name)\n",
    "                     for headline in headlines]\n",
    "matched_cities = [get_name_in_text(headline, city_to_name)\n",
    "                  for headline in headlines]\n",
    "data = {'Headline': headlines, 'City': matched_cities, \n",
    "        'Country': matched_countries}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore our location table. We'll start by summarizing the contents of `df` using the `describe` method.\n",
    "\n",
    "**Summarizing the location data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df[['City', 'Country']].describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequently mentioned city is apparently __Of, Turkey__. The 45 instances of __Of__ are more likely\n",
    "to match the preposition than the rarely referenced Turkish location. We will output some instances of __Of__ in order to confirm the error.\n",
    "\n",
    "**Fetching cities named __Of__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_cities = df[df.City == 'Of'][['City', 'Headline']]\n",
    "ten_of_cities = of_cities.head(10)\n",
    "print(ten_of_cities.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the wrongly matched headlines we matched to __Of__ but not to the actual city\n",
    "name. The mismatches occurred because we didn't consider potential multiple matches in a headline. How frequently do headlines contain two or more city matches? Lets find out. \n",
    "\n",
    "**Finding multi-city headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cities_in_headline(headline):\n",
    "    cities_in_headline = set()\n",
    "    for regex, name in city_to_name.items():          \n",
    "        match = regex.search(headline)\n",
    "        if match:\n",
    "            if headline[match.start()].isupper():\n",
    "                cities_in_headline.add(name)\n",
    "                \n",
    "    return list(cities_in_headline)\n",
    "\n",
    "df['Cities'] = df['Headline'].apply(get_cities_in_headline)\n",
    "df['Num_cities'] = df['Cities'].apply(len)\n",
    "df_multiple_cities = df[df.Num_cities > 1]\n",
    "num_rows, _ = df_multiple_cities.shape\n",
    "print(f\"{num_rows} headlines match multiple cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67 headlines contain more than one city, representing approximately 10% of the data. Why are so many headlines matching against multiple locations? Perhaps exploring some sample matches will yield an answer.\n",
    "\n",
    "**Sampling multi-city headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_cities = df_multiple_cities[['Cities', 'Headline']].head(10)\n",
    "print(ten_cities.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short, invalid city names are getting matched to the headlines along with longer, more correct location names. One solution is simply to assign the longest city-name as the representative location if more than one matched city is found.\n",
    "\n",
    "**Selecting the longest city names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_city(cities):\n",
    "    if cities:\n",
    "        return max(cities, key=len)\n",
    "    return None\n",
    "\n",
    "df['City'] = df['Cities'].apply(get_longest_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we'll output those rows in the the table that contain a short city-name (4 characters or less), in order to ensure that no erroneous short name is getting assigned to one of our headlines.\n",
    "\n",
    "**Printing the shortest city names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_cities = df[df.City.str.len() <= 4][['City', 'Headline']]\n",
    "print(short_cities.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now shift our attention from cities to countries. Only 15 of the total headlines contain actual country information. The count is low enough for us to manually examine all these headlines.\n",
    "\n",
    "**Fetching headlines with countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries = df[df.Country.notnull()][['City', \n",
    "                                         'Country', \n",
    "                                         'Headline']]\n",
    "print(df_countries.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the country-bearing headlines also contain city information. Thus, we can assign a latitude and longitude without relying on the country's central coordinates. Consequently, we can disregard the country names from our analysis.\n",
    "\n",
    "**Dropping countries from the table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Country', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nearly ready to add latitudes and longitudes to our table. However, we first need to consider those rows where no locations were detected. Lets count the number of unmatched headlines, and then print a subset of that data.\n",
    "\n",
    "**Exploring unmatched headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unmatched = df[df.City.isnull()]\n",
    "num_unmatched = len(df_unmatched)\n",
    "print(f\"{num_unmatched} headlines contain no city matches.\")\n",
    "print(df_unmatched.head(10)[['Headline']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 6% of the headlines did not match any cities. Given that low frequency, we will delete the missing mentions.\n",
    "\n",
    "**Dropping unmatched headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.City.isnull()][['City', 'Headline']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Clustering the Extracted Location Data\n",
    "\n",
    "All the rows in our table contain a city-name. Now, we can assign a latitude and longitude to each row.\n",
    "\n",
    "**Assigning geographic coordinates to cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes, longitudes = [], []\n",
    "for city_name in df.City.values:\n",
    "    city = max(gc.get_cities_by_name(city_name), \n",
    "              key=lambda x: list(x.values())[0]['population'])\n",
    "    city = list(city.values())[0]\n",
    "    latitudes.append(city['latitude']) \n",
    "    longitudes.append(city['longitude'])\n",
    "\n",
    "df = df.assign(Latitude=latitudes, Longitude=longitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute K-means across our set of 2D coordinates. We'll use the Elbow method to choose a reasonable value for K.\n",
    "\n",
    "**Plotting a geographic elbow curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = df[['Latitude', 'Longitude']].values\n",
    "k_values = range(1, 10)\n",
    "inertia_values = []\n",
    "for k in k_values:\n",
    "    inertia_values.append(KMeans(k).fit(coordinates).inertia_)\n",
    "\n",
    "plt.plot(range(1, 10), inertia_values)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"elbow\" within our Elbow plot points to a K of 3 That K-value is very low; limiting our scope to at-most 3 different geographic territories.\n",
    "\n",
    "**Using K-means to cluster cities into three groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(clusters, longitudes, latitudes):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = plt.axes(projection=PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.scatter(longitudes, latitudes, c=clusters)\n",
    "    ax.set_global()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df['Cluster'] = KMeans(3).fit_predict(coordinates)\n",
    "plot_clusters(df.Cluster, df.Longitude, df.Latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These continental categories are too broad to actually be useful. Perhaps our K was too low after all. We'll disregard the recommended K-value from the Elbow analysis, and double the size of K to 6.\n",
    "\n",
    "**Using K-means to cluster cities into six groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = KMeans(6).fit_predict(coordinates)\n",
    "plot_clusters(df.Cluster, df.Longitude, df.Latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-mean's sense of centrality is unable to properly distinguish between Africa, Europe and Asia. As an alternative approach, we can attempt to execute DBSCAN clustering. The DBSCAN algorithm takes as input any distance metric\n",
    "of our choosing, allowing us to cluster on the great-circle distance between points.\n",
    "\n",
    "**Defining a NumPy-based great-circle metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def great_circle_distance(coord1, coord2, radius=3956):\n",
    "    if np.array_equal(coord1, coord2):\n",
    "        return 0.0 \n",
    "\n",
    "    coord1, coord2 = np.radians(coord1), np.radians(coord2)\n",
    "    delta_x, delta_y = coord2 - coord1\n",
    "    haversin = sin(delta_x / 2) ** 2 + np.product([cos(coord1[0]),\n",
    "                                                   cos(coord2[0]), \n",
    "                                                   sin(delta_y / 2) ** 2])\n",
    "    return  2 * radius * asin(haversin ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our distance metric in place, we are ready to run the DBSCAN algorithm. \n",
    "\n",
    "**Using DBSCAN to cluster cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = great_circle_distance\n",
    "dbscan = DBSCAN(eps=250, min_samples=3, metric=metric)\n",
    "df['Cluster'] = dbscan.fit_predict(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN assigns -1 to outlier data-points that do not cluster. Lets remove these outliers from our table. Afterwards, we’ll plot the remaining results.\n",
    "\n",
    "**Plotting non-outlier DBSCAN clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df[df.Cluster > -1]\n",
    "plot_clusters(df_no_outliers.Cluster, df_no_outliers.Longitude,\n",
    "              df_no_outliers.Latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN has does a decent job of generating discrete clusters within parts of South America, Asia, and Southern Africa. The Eastern United States however, falls into a single overly-dense cluster. Lets cluster US locations independently from the rest of the World. To do so, we will first assign country-codes across each of our cities.\n",
    "\n",
    "**Assigning country codes to cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_code(city_name):\n",
    "    city = max(gc.get_cities_by_name(city_name), \n",
    "               key=lambda x: list(x.values())[0]['population'])\n",
    "    return list(city.values())[0]['countrycode']\n",
    "\n",
    "df['Country_code'] = df.City.apply(get_country_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The country-codes allow us to separate the data into two distinct `DataFrame` objects. The first object, `df_us`,  which hold all the United States locations. The second object, `df_not_us`, will hold all the remaining global cities.\n",
    "\n",
    "**Separating US and global cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df[df.Country_code == 'US']\n",
    "df_not_us = df[df.Country_code != 'US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've separated US and non-US cities. Now, we will need to re-cluster the coordinates within the two separated tables. \n",
    "\n",
    "**Re-clustering extracted cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_cluster(input_df, eps):\n",
    "    input_coord = input_df[['Latitude', 'Longitude']].values\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3, \n",
    "                    metric=great_circle_distance)\n",
    "    clusters = dbscan.fit_predict(input_coord)\n",
    "    input_df = input_df.assign(Cluster=clusters)\n",
    "    return input_df[input_df.Cluster > -1]\n",
    "\n",
    "df_not_us = re_cluster(df_not_us, 250)\n",
    "df_us = re_cluster(df_us, 125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Insights from Location Clusters\n",
    "\n",
    "Lets investigate the clustered data within the `df_not_us` table. We'll start by grouping the clustered results using the Pandas `groupby` method.\n",
    "\n",
    "**Grouping cities by cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_not_us.groupby('Cluster')\n",
    "num_groups = len(groups)\n",
    "print(f\"{num_groups} Non-US have been clusters detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31 global clusters have been detected. Lets sort these groups by size and count the headlines in the largest cluster.\n",
    "\n",
    "**Finding the largest cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_groups = sorted(groups, key=lambda x: len(x[1]), \n",
    "                       reverse=True)\n",
    "group_id, largest_group = sorted_groups[0]\n",
    "group_size = len(largest_group)\n",
    "print(f\"Largest cluster contains {group_size} headlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest cluster contains 51 total headlines. Reading all these headlines individually will be a time-consuming process. We can save time by outputting just those headlines that represent the most central locations in the cluster.\n",
    "\n",
    "**Computing cluster centrality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centrality(group):\n",
    "    group_coords = group[['Latitude', 'Longitude']].values\n",
    "    center = group_coords.mean(axis=0)\n",
    "    distance_to_center = [great_circle_distance(center, coord)\n",
    "                          for coord in group_coords]\n",
    "    group['Distance_to_center'] = distance_to_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the centrality allows us to sort the grouped locations based on their distance to the centers, in order to output the most central headlines. Lets print the 5 most central headlines within our largest cluster.\n",
    "\n",
    "**Finding the central headlines in largest cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_centrality(group):\n",
    "    compute_centrality(group)\n",
    "    return group.sort_values('Distance_to_center', ascending=True)\n",
    "\n",
    "largest_group = sort_by_centrality(largest_group)\n",
    "for headline in largest_group.Headline.values[:5]:\n",
    "    print(headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central headlines in largest_cluster focus on an outbreak of Mad Cow Disease within various European cities. We can confirm that the cluster’s locale is centered in Europe by outputting the top countries associated with cities in the cluster.\n",
    "\n",
    "**Finding the top three countries in largest cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def top_countries(group):\n",
    "    countries = [gc.get_countries()[country_code]['name']\n",
    "                 for country_code in group.Country_code.values]\n",
    "    return Counter(countries).most_common(3)\n",
    "\n",
    "\n",
    "print(top_countries(largest_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets repeat this analysis across the next four largest global clusters.\n",
    "\n",
    "**Summarizing content within the largest clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, group in sorted_groups[1:5]:\n",
    "    sorted_group = sort_by_centrality(group)\n",
    "    print(top_countries(sorted_group))\n",
    "    for headline in sorted_group.Headline.values[:5]:\n",
    "        print(headline)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets turn our attention to the US clusters. We'll start by visualizing the clusters on a map of the United States.\n",
    "\n",
    "**Plotting United States DBSCAN clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "ax = plt.axes(projection=LambertConformal())\n",
    "ax.set_extent(us_extent)\n",
    "ax.scatter(df_us.Longitude, df_us.Latitude, c=df_us.Cluster, \n",
    "\t\t   transform=PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.STATES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualized map yields reasonable outputs. We'll proceed to analyze the top 5 US clusters by printing their centrality-sorted headlines.\n",
    "\n",
    "**Summarizing content within the largest US clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_groups = df_us.groupby('Cluster')\n",
    "us_sorted_groups = sorted(us_groups, key=lambda x: len(x[1]),\n",
    "                         reverse=True)\n",
    "for _, group in us_sorted_groups[:5]:\n",
    "    sorted_group = sort_by_centrality(group)\n",
    "    for headline in sorted_group.Headline.values[:5]:\n",
    "        print(headline)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot one additional image. It will summarize the menacing scope of the spreading Zika epidemic. The image will display all US and global clusters where Zika is mentioned in more than 50% of article headlines.\n",
    "\n",
    "**Plotting Zika clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zika_mentions(headlines):\n",
    "    zika_regex = re.compile(r'\\bzika\\b', \n",
    "                            flags=re.IGNORECASE)\n",
    "    zika_count = 0\n",
    "    for headline in headlines:\n",
    "        if zika_regex.search(headline): \n",
    "            zika_count += 1\n",
    "    \n",
    "    return zika_count\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = plt.axes(projection=PlateCarree())\n",
    "\n",
    "for _, group in sorted_groups + us_sorted_groups:\n",
    "    headlines = group.Headline.values\n",
    "    zika_count = count_zika_mentions(headlines)\n",
    "    if float(zika_count) / len(headlines) > 0.5:\n",
    "        ax.scatter(group.Longitude, group.Latitude)\n",
    "\n",
    "ax.coastlines()\n",
    "ax.set_global()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
