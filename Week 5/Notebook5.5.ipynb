{"cells":[{"cell_type":"markdown","metadata":{},"source":["# AAI614: Data Science & its Applications\n","\n","*Notebook 5.5: Classify this Disease*\n","\n","<a href=\"https://colab.research.google.com/github/harmanani/AAI614/blob/main/Week%205/Notebook5.5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{},"source":["# Example I: Diseases Classification\n","\n","This problem applies machine learning to the field of Medical Science and making the task of Physician easy is the main purpose of this dataset. This dataset has 132 parameters on which 42 different types of diseases can be predicted."]},{"cell_type":"markdown","metadata":{},"source":["#### Import Libraries"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.448828Z","iopub.status.busy":"2023-01-21T15:27:06.448355Z","iopub.status.idle":"2023-01-21T15:27:06.454873Z","shell.execute_reply":"2023-01-21T15:27:06.453346Z","shell.execute_reply.started":"2023-01-21T15:27:06.448794Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["The Dataset consists of 2 CSV files: One of them is training and other is for testing your model.  Each CSV file has 133 columns. 132 of these columns are symptoms that a person experiences and last column is the prognosis.  These symptoms are mapped to 42 diseases you can classify these set of symptoms to."]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.618271Z","iopub.status.busy":"2023-01-21T15:27:06.617512Z","iopub.status.idle":"2023-01-21T15:27:06.695976Z","shell.execute_reply":"2023-01-21T15:27:06.694873Z","shell.execute_reply.started":"2023-01-21T15:27:06.618217Z"},"trusted":true},"outputs":[],"source":["import ssl\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","train=pd.read_csv(\"https://raw.githubusercontent.com/harmanani/AAI614/main/Week%205/Training.csv\")\n","test=pd.read_csv(\"https://raw.githubusercontent.com/harmanani/AAI614/main/Week%205/Testing.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Quick EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.699228Z","iopub.status.busy":"2023-01-21T15:27:06.698327Z","iopub.status.idle":"2023-01-21T15:27:06.722058Z","shell.execute_reply":"2023-01-21T15:27:06.720595Z","shell.execute_reply.started":"2023-01-21T15:27:06.699179Z"},"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.797739Z","iopub.status.busy":"2023-01-21T15:27:06.797312Z","iopub.status.idle":"2023-01-21T15:27:06.82092Z","shell.execute_reply":"2023-01-21T15:27:06.81965Z","shell.execute_reply.started":"2023-01-21T15:27:06.797703Z"},"trusted":true},"outputs":[],"source":["test.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### Drop Unwanted Columns"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.825346Z","iopub.status.busy":"2023-01-21T15:27:06.823258Z","iopub.status.idle":"2023-01-21T15:27:06.83844Z","shell.execute_reply":"2023-01-21T15:27:06.836673Z","shell.execute_reply.started":"2023-01-21T15:27:06.825293Z"},"trusted":true},"outputs":[],"source":["train=train.drop([\"Unnamed: 133\"],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.888241Z","iopub.status.busy":"2023-01-21T15:27:06.887649Z","iopub.status.idle":"2023-01-21T15:27:06.898569Z","shell.execute_reply":"2023-01-21T15:27:06.897295Z","shell.execute_reply.started":"2023-01-21T15:27:06.888192Z"},"trusted":true},"outputs":[],"source":["train.prognosis.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.901184Z","iopub.status.busy":"2023-01-21T15:27:06.90061Z","iopub.status.idle":"2023-01-21T15:27:06.915728Z","shell.execute_reply":"2023-01-21T15:27:06.914484Z","shell.execute_reply.started":"2023-01-21T15:27:06.90115Z"},"trusted":true},"outputs":[],"source":["train.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:06.977991Z","iopub.status.busy":"2023-01-21T15:27:06.977286Z","iopub.status.idle":"2023-01-21T15:27:06.990258Z","shell.execute_reply":"2023-01-21T15:27:06.988872Z","shell.execute_reply.started":"2023-01-21T15:27:06.97795Z"},"trusted":true},"outputs":[],"source":["test.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["#### Split Dataset"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:07.008132Z","iopub.status.busy":"2023-01-21T15:27:07.006908Z","iopub.status.idle":"2023-01-21T15:27:07.019919Z","shell.execute_reply":"2023-01-21T15:27:07.01861Z","shell.execute_reply.started":"2023-01-21T15:27:07.008089Z"},"trusted":true},"outputs":[],"source":["P = train[[\"prognosis\"]]\n","X = train.drop([\"prognosis\"],axis=1)\n","Y = test.drop([\"prognosis\"],axis=1)"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:07.143278Z","iopub.status.busy":"2023-01-21T15:27:07.142105Z","iopub.status.idle":"2023-01-21T15:27:07.157799Z","shell.execute_reply":"2023-01-21T15:27:07.156354Z","shell.execute_reply.started":"2023-01-21T15:27:07.143236Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","xtrain,xtest,ytrain,ytest = train_test_split(X,P,test_size=0.2,random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Classification"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:07.233975Z","iopub.status.busy":"2023-01-21T15:27:07.23262Z","iopub.status.idle":"2023-01-21T15:27:07.238433Z","shell.execute_reply":"2023-01-21T15:27:07.237172Z","shell.execute_reply.started":"2023-01-21T15:27:07.233929Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-21T15:27:07.243136Z","iopub.status.busy":"2023-01-21T15:27:07.242554Z","iopub.status.idle":"2023-01-21T15:27:07.966328Z","shell.execute_reply":"2023-01-21T15:27:07.96507Z","shell.execute_reply.started":"2023-01-21T15:27:07.2431Z"},"trusted":true},"outputs":[],"source":["rf= RandomForestClassifier(random_state=42)\n","model_rf = rf.fit(xtrain,ytrain)\n","tr_pred_rf = model_rf.predict(xtrain)\n","ts_pred_rf = model_rf.predict(xtest)\n","\n","print(\"training accuracy is:\",accuracy_score(ytrain,tr_pred_rf))\n","print(\"testing accuracy is:\",accuracy_score(ytest,ts_pred_rf))"]},{"cell_type":"markdown","metadata":{},"source":["### Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test.join(pd.DataFrame(model_rf.predict(Y),columns=[\"predicted\"]))[[\"prognosis\",\"predicted\"]]"]},{"cell_type":"markdown","metadata":{},"source":["# Example II: Heart Disease Cleveland"]},{"cell_type":"markdown","metadata":{},"source":["### Data Dictionary\n","\n","A data dictionary describes the data you're dealing with.\n","\n","Not all datasets come with them so this is where you may have to do your research or ask a **subject matter expert** (someone who knows about the data) for more.\n","\n","The following are the features we'll use to predict our target variable (heart disease or no heart disease).\n","\n","| Feature  | Description | Example Values |\n","|:-----|:-----|:------|\n","| **age** | Age in years | 29, 45, 60 |\n","| **sex** | 1 = male; 0 = female | 0, 1  |\n","| **cp**  | Chest pain type | 0: Typical angina (chest pain), 1: Atypical angina (chest pain not related to heart), 2: Non-anginal pain (typically esophageal spasms (non heart related), 3: Asymptomatic (chest pain not showing signs of disease) |\n","| **trestbps** | Resting blood pressure (in mm Hg on admission to the hospital)  | 120, 140, 150 |\n","| **chol** | Serum cholesterol in mg/dl | 180, 220, 250 |\n","| **fbs** | Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) | 0, 1 |\n","| **restecg** | Resting electrocardiographic results | 0: Nothing to note, 1: ST-T Wave abnormality, 2: Left ventricular hypertrophy  |\n","| **thalach** | Maximum heart rate achieved | 160, 180, 190 |\n","| **exang**  | Exercise induced angina (1 = yes; 0 = no) | 0, 1 |\n","| **oldpeak**  | ST depression (heart potentially not getting enough oxygen) induced by exercise relative to rest | 0.5, 1.0, 2.0  |\n","| **slope** | The slope of the peak exercise ST segment | 0: Upsloping, 1: Flatsloping, 2: Downsloping |\n","| **ca** | Number of major vessels (0-3) colored by fluoroscopy | 0, 1, 2, 3 |\n","| **thal** | Thalium stress result  | 1: Normal, 3: Normal, 6: Fixed defect, 7: Reversible defect |\n","| **target** | Have disease or not (1 = yes; 0 = no) | 0, 1 |\n","\n","> **Note:** No personal identifiable information (PPI) can be found in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Regular EDA and plotting libraries\n","import numpy as np # np is short for numpy\n","\n","import pandas as pd # pandas is so commonly used, it's shortened to pd\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import seaborn as sns # seaborn gets shortened to sns, TK - can seaborn be removed for matplotlib (simpler)?\n","\n","## Models\n","import sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","## Model evaluators\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","# from sklearn.metrics import plot_roc_curve # note: this was changed in Scikit-Learn 1.2+ to be \"RocCurveDisplay\" (see below)\n","from sklearn.metrics import RocCurveDisplay # new in Scikit-Learn 1.2+\n","\n","# Print last updated\n","import datetime\n","print(f\"Notebook last updated: {datetime.datetime.now()}\\n\")\n","\n","# Print versions of libraries we're using (as long as yours are equal or greater than these, your code should work)\n","print(f\"NumPy version: {np.__version__}\")\n","print(f\"pandas version: {pd.__version__}\")\n","print(f\"matplotlib version: {matplotlib.__version__}\")\n","print(f\"Scikit-Learn version: {sklearn.__version__}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"https://raw.githubusercontent.com/harmanani/AAI614/main/Week%205/heart-disease.csv\")\n","df.shape"]},{"cell_type":"markdown","metadata":{},"source":["## EDA Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.target.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.target.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Let us compare two columns using the function [`pd.crosstab(index, columns)`](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html).\n","\n","This is helpful if you want to start gaining an intuition about how your independent variables interact with your dependent variables.  Let's compare our target column with the sex column.  \n","Remember from our data dictionary, for the target column, 1 = heart disease present, 0 = no heart disease. And for sex, 1 = male, 0 = female."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.sex.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compare target column with sex column\n","pd.crosstab(index=df.target, columns=df.sex)"]},{"cell_type":"markdown","metadata":{},"source":["Since there are about 100 women and 72 of them have a positive value of heart disease being present, we might infer, based on this one variable if the participant is a woman, there's a ~72% (72/96 women in our dataset are positive for heart disease) chance she has heart disease.\n","\n","As for males, there's about 200 total with around half (93/207) indicating a presence of heart disease.\n","So we might predict, if the participant is male, 50% of the time he will have heart disease.\n","\n","Averaging these two values, we can assume, based on no other parameters, if there's a person, there's a 62.5% chance they have heart disease.\n","\n","This can be our very simple **baseline**, we'll try to beat it with machine learning."]},{"cell_type":"markdown","metadata":{},"source":["Let us now visualize our data.  We can plot our `pd.crosstab` comparison by calling the `plot()` method and passing it a few parameters:\n","\n","* `kind`- The type of plot you want (e.g. `\"bar\"` for a bar plot).\n","* `figsize=(length, width)` - How big you want it to be.\n","* `color=[colour_1, colour_2]` - The different colours you'd like to use."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a plot\n","pd.crosstab(df.target, df.sex).plot(kind=\"bar\", figsize=(10,6), color=[\"salmon\", \"lightblue\"])\n","\n","# Add some attributes to it\n","plt.title(\"Heart Disease Frequency vs Sex\")\n","plt.xlabel(\"0 = No Disease, 1 = Disease\")\n","plt.ylabel(\"Amount\")\n","plt.legend([\"Female\", \"Male\"])\n","plt.xticks(rotation=0); # keep the labels on the x-axis vertical"]},{"cell_type":"markdown","metadata":{},"source":["### Comparing age and maximum heart rate\n","\n","Let's try combining a couple of independent variables, such as, `age` and `thalach` (maximum heart rate) and then comparing them to our target variable `heart disease`.\n","\n","Because there are so many different values for `age` and `thalach`, we'll use a scatter plot."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create another figure\n","plt.figure(figsize=(10,6))\n","\n","# Start with positve examples\n","plt.scatter(df.age[df.target==1],\n","            df.thalach[df.target==1],\n","            c=\"salmon\") # define it as a scatter figure\n","\n","# Now for negative examples, we want them on the same plot, so we call plt again\n","plt.scatter(df.age[df.target==0],\n","            df.thalach[df.target==0],\n","            c=\"lightblue\") # axis always come as (x, y)\n","\n","# Add some helpful info\n","plt.title(\"Heart Disease in function of Age and Max Heart Rate\")\n","plt.xlabel(\"Age\")\n","plt.legend([\"Disease\", \"No Disease\"])\n","plt.ylabel(\"Max Heart Rate\");"]},{"cell_type":"markdown","metadata":{},"source":["What can we infer from this?\n","\n","It seems the younger someone is, the higher their max heart rate (dots are higher on the left of the graph) and it seems there may be more heart disease in the younger population too (more orange dots).\n","\n","Both of these are observational of course, but this is what we're trying to do, build an understanding of the data.\n","\n","Let's check the age **distribution**.\n","\n","> **Note:** Distribution can considered as the *spread* of data. As in, when viewed as a whole, what different values appear in the data?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Histograms are a great way to check the distribution of a variable\n","df.age.plot.hist();"]},{"cell_type":"markdown","metadata":{},"source":["We can see it's a [**normal distribution**](https://en.wikipedia.org/wiki/Normal_distribution) but slightly swaying to the right, which reflects in the scatter plot above."]},{"cell_type":"markdown","metadata":{},"source":["### Comparing heart disease frequency and chest pain type\n","\n","Let's try comparing another independent variable with our target variable.\n","\n","This time, we'll use `cp` (chest pain) as the independent variable.\n","\n","We'll use the same process as we did before with `sex`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.crosstab(index=df.cp, columns=df.target)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a new crosstab and base plot\n","pd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n","                                   figsize=(10,6),\n","                                   color=[\"lightblue\", \"salmon\"])\n","\n","# Add attributes to the plot to make it more readable\n","plt.title(\"Heart Disease Frequency Per Chest Pain Type\")\n","plt.xlabel(\"Chest Pain Type\")\n","plt.ylabel(\"Frequency\")\n","plt.legend([\"No Disease\", \"Disease\"])\n","plt.xticks(rotation = 0);"]},{"cell_type":"markdown","metadata":{},"source":["What can we infer from this?\n","\n","Remember from our data dictionary what the different levels of chest pain are.\n","\n","| Feature  | Description | Example Values |\n","|:-----|:-----|:------|\n","| **cp**  | Chest pain type | 0: Typical angina (chest pain), 1: Atypical angina (chest pain not related to heart), 2: Non-anginal pain (typically esophageal spasms (non heart related), 3: Asymptomatic (chest pain not showing signs of disease) |\n","    \n","It's interesting that atypical angina (value 1) states it's not related to the heart but seems to have a higher ratio of participants with heart disease than not.\n","\n","Wait...?\n","\n","What does *atypical agina* even mean?\n","\n","At this point, it's important to remember, if your data dictionary doesn't supply you enough information, you may want to do further research on your values.\n","\n","This research may come in the form of asking a **subject matter expert** (such as a cardiologist or the person who gave you the data) or Googling to find out more.\n","\n","According to PubMed, it seems [even some medical professionals are confused by the term](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2763472/).\n","\n","> Today, 23 years later, “atypical chest pain” is still popular in medical circles. Its meaning, however, remains unclear. A few articles have the term in their title, but do not define or discuss it in their text. In other articles, the term refers to noncardiac causes of chest pain.\n","\n","Although not conclusive, the plot above is a sign there may be a confusion of defintions being represented in data."]},{"cell_type":"markdown","metadata":{},"source":["## Correlation between independent variables\n","\n","Finally, we'll compare all of the independent variables in one hit.\n","\n","Why?\n","\n","Because this may give an idea of which independent variables may or may not have an impact on our target variable.\n","\n","We can do this using [`pd.DataFrame.corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) which will create a [**correlation matrix**](https://en.wikipedia.org/wiki/Correlation#Correlation_matrices) for us, in other words, a big table of numbers telling us how related each variable is the other."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find the correlation between our independent variables\n","corr_matrix = df.corr()\n","corr_matrix"]},{"cell_type":"markdown","metadata":{},"source":["Following the data explorer's motto of *visualize, visualize, visualize!*, let's plot this matrix."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's make it look a little prettier\n","corr_matrix = df.corr()\n","plt.figure(figsize=(15, 10))\n","sns.heatmap(corr_matrix,\n","            annot=True,\n","            linewidths=0.5,\n","            fmt= \".2f\",\n","            cmap=\"YlGnBu\");"]},{"cell_type":"markdown","metadata":{},"source":["Much better. A higher positive value means a potential positive correlation (increase) and a higher negative value means a potential negative correlation (decrease)."]},{"cell_type":"markdown","metadata":{},"source":["### Notes\n","\n","* Not every EDA will look the same, what we've seen here is an example of what you could do for structured, tabular dataset.\n","* You don't necessarily have to do the same plots as we've done here, there are many more ways to visualize data, I encourage you to look at more.\n","* Quite often, we'll want to find:\n","    * **Distributions** - What's the spread of the data? We can do this with [`pd.DataFrame.hist(column=\"target_column\")`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html).\n","    * **Missing values** - Is our data missing anything? Why might this be the case and will this affect us going forward? We can do this with [`pd.DataFrame.info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) or [`pd.isnull()`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html).\n","    * **Outliers** - Are there any samples that lay quite far outside the rest of our data's distributions? How might these affect the data going forward?\n","\n","With this being said, let's build some models!"]},{"cell_type":"markdown","metadata":{},"source":["## Build Our Models\n","\n","We've explored the data, now we'll try to build a machine learning model to be able to predict our target variable based on the 13 independent variables.\n","\n","Our problem?\n","\n","> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n","\n","That's what we'll be trying to answer.\n","\n","Our evaluation metric?\n","\n","> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursure this project.\n","\n","That's what we'll be aiming for.\n","\n","But before we build a model, we have to get our dataset ready.\n","\n","Let's look at it again."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["We're trying to predict our target variable using all of the other variables.\n","\n","To do this, we'll split the target variable from the rest.\n","\n","We can do this by creating:\n","\n","* `X` - Our features (all variables except the `target` column) using [`pd.DataFrame.drop(labels=\"target\")`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html).\n","* `y` - Our target variable using [`df.target.to_numpy()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy) (this will extract the `target` column as a NumPy array)."]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["# Everything except target variable\n","X = df.drop(labels=\"target\", axis=1)\n","\n","# Target variable\n","y = df.target.to_numpy()"]},{"cell_type":"markdown","metadata":{},"source":["Let's see our new variables."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Independent variables (no target column)\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Targets (in the form of a NumPy array)\n","y, type(y)"]},{"cell_type":"markdown","metadata":{},"source":["Split the data into two parts.  The `test_size` parameter is used to tell the `train_test_split()` function how much of our data we want in the test set.\n","\n","For our problem, a train and test set are enough. But for other problems, you could also use a validation (train/validation/test) set or cross-validation (we'll see this later on)."]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["# Random seed for reproducibility (since train_test_split is random by default, setting the seed will create reproducible splits)\n","np.random.seed(42)\n","\n","# Split into train & test set\n","X_train, X_test, y_train, y_test = train_test_split(X, # independent variables\n","                                                    y, # dependent variable\n","                                                    test_size = 0.2) # percentage of data to use for test set"]},{"cell_type":"markdown","metadata":{},"source":["Let's look at our training data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_train, len(y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Beautiful, we can see we're using 242 samples to train on.\n","\n","Let's look at our test data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_test, len(y_test)"]},{"cell_type":"markdown","metadata":{},"source":["And we've got 61 examples we'll test our model(s) on.\n","\n","Let's build some."]},{"cell_type":"markdown","metadata":{},"source":["### Choosing a model\n","\n","Let us start the following models:\n","\n","1. Logistic Regression - [`sklearn.linear_model.LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n","2. K-Nearest Neighbors - [`sklearn.neighbors.KNeighboursClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n","3. RandomForest - [`sklearn.ensemble.RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"]},{"cell_type":"markdown","metadata":{},"source":["#### Why these?\n","\n","If we look at the [Scikit-Learn algorithm machine learning model map](https://scikit-learn.org/stable/machine_learning_map.html), we can see we're working on a classification problem and these are the algorithms it suggests (plus a few more).\n","\n","| <img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-ml-map-cheatsheet-heart-disease-ensemble.png?raw=1\" alt=\"an example classification path using the Scikit-Learn machine learning model map\" width=500/> |\n","|:--:|\n","| An example path we can take using the Scikit-Learn Machine Learning Map |\n","\n","> \"Wait, I don't see Logistic Regression and why not use LinearSVC?\"\n","\n","Good questions.\n","\n","I was confused too when I didn't see Logistic Regression listed as well because when you read the Scikit-Learn documentation on it, you can see it's [a model for classification](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).\n","\n","And as for [`sklearn.svm.LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html), let's pretend we've tried it (you can try it for yourself if you like), and it doesn't work, so we're following other options in the map.\n","\n","For now, knowing each of these algorithms inside and out is not essential (however, this would be a fantastic extension to this project).\n","\n","Machine learning and data science is an iterative practice.\n","\n","These algorithms are tools in your toolbox.\n","\n","In the beginning, on your way to becoming a practitioner, it's more important to understand your problem (such as, classification versus regression) and what tools you can use to solve it.\n","\n","Since our dataset is relatively small, we can run some quick experiments to see which algorithm performs best and iteratively try to improve it.\n","\n","Many of the algorithms in the Scikit-Learn library have similar APIs (Application Programming Interfaces).\n","\n","For example, for training a model you can use [`model.fit(X_train, y_train)`](https://scikit-learn.org/stable/glossary.html#term-fit).\n","\n","And for scoring a model [`model.score(X_test, y_test)`](https://scikit-learn.org/stable/glossary.html#term-score) (scoring a model compares predictions to the ground truth labels).\n","\n","For classification models, calling `score()` usually defaults to returning the ratio (accuracy) of correct predictions (1.0 = 100% correct).\n","\n","Since the algorithms we've chosen implement the same methods for fitting them to the data as well as evaluating them, let's put them in a dictionary and create a which fits and scores them."]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[],"source":["# Put models in a dictionary\n","models = {\"KNN\": KNeighborsClassifier(),\n","          \"Logistic Regression\": LogisticRegression(max_iter=100), # Note: if you see a warning about \"convergence not reached\", you can increase `max_iter` until convergence is reached\n","          \"Random Forest\": RandomForestClassifier()}\n","\n","# Create function to fit and score models\n","def fit_and_score(models, X_train, X_test, y_train, y_test):\n","    \"\"\"\n","    Fits and evaluates given machine learning models.\n","    models : a dict of different Scikit-Learn machine learning models\n","    X_train : training data\n","    X_test : testing data\n","    y_train : labels assosciated with training data\n","    y_test : labels assosciated with test data\n","    \"\"\"\n","    # Random seed for reproducible results\n","    np.random.seed(42)\n","    # Make a list to keep model scores\n","    model_scores = {}\n","    # Loop through models\n","    for name, model in models.items():\n","        # Fit the model to the data\n","        model.fit(X_train, y_train)\n","        # Evaluate the model and append its score to model_scores\n","        model_scores[name] = model.score(X_test, y_test)\n","    return model_scores"]},{"cell_type":"markdown","metadata":{},"source":["Function built!\n","\n","Now let's see how our collection of models go on our data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_scores = fit_and_score(models=models,\n","                             X_train=X_train,\n","                             X_test=X_test,\n","                             y_train=y_train,\n","                             y_test=y_test)\n","model_scores"]},{"cell_type":"markdown","metadata":{},"source":["It looks like each of our models was able to fit our data without any errors.\n","\n","How about we compare them visually?"]},{"cell_type":"markdown","metadata":{},"source":["### Comparing the results of several models\n","\n","Since we've saved our models scores to a dictionary, we can plot them by first converting them to a DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n","model_compare.T.plot.bar();"]},{"cell_type":"markdown","metadata":{},"source":["From the plot it looks like the [`sklearn.linear_model.LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model performs best.\n","\n","Now... since we've found the best model.\n","\n","Let's take it to the boss and show her what we've found!"]},{"cell_type":"markdown","metadata":{},"source":["## Taking our best model to the boss\n","\n","\n","| **Term** | **Definition** |\n","| :----- | :----- |\n","| [**Confusion matrix**](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) | Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). |\n","| [**Cross-validation**](https://scikit-learn.org/stable/modules/cross_validation.html) | Splits your dataset into multiple versions of training and test sets and trains/evaluations your model on each different version. This ensures that your evaluation metrics are across several different splits of data rather than a single split (if it was only a single split, you might get lucky and get better than usual results, the same for the reverse, if you get a poor split, you might find your metrics lower than they should be). |\n","| [**Precision**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) | A common classification evaluation metric. Measures the proportion of true positives over total number of samples. Higher precision leads to fewer false positives. |  \n","[**Recall**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) | A common classification evaluation metric. Measures the proportion of true positives over total number of true positives and false negatives. Higher recall leads to fewer false negatives. |\n","| [**F1 score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) | Combines precision and recall into one metric. 1 is best, 0 is worst. |\n","| [**Classification report**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) | Sklearn has a built-in function called `classification_report()` which returns some of the main classification metrics such as precision, recall and f1-score. |\n","| [**ROC Curve**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) | [Receiver Operating Characterisitc](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a plot of true positive rate versus false positive rate. A perfect curve will follow the left and top border of a plot. |\n","| [**Area Under Curve (AUC)**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) | The area underneath the ROC curve. A perfect model achieves a score of 1.0. |"]},{"cell_type":"markdown","metadata":{},"source":["### 6.1 Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand\n","\n","There are several hyperparameters we can tune for the K-Nearest Neighbors (KNN) algorithm (or [`sklearn.neighbors.KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)).\n","\n","But for now, let's start with one, the number of neighbors.\n","\n","The default is 5 (`n_neigbors=5`).\n","\n","What are neighbours?\n","\n","Well, imagine all our different samples on one graph like the scatter graph several cells above.\n","\n","KNN works by assuming dots which are closer together belong to the same class.\n","\n","If `n_neighbors=5` then it assume a dot with the 5 closest dots around it are in the same class.\n","\n","We've left out some details here like what defines close or how distance is calculated but I encourage you to research them by going through the documentation.\n","\n","For now, let's try a few different values of `n_neighbors` and test how the results go."]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[],"source":["# Create a list of train scores\n","train_scores = []\n","\n","# Create a list of test scores\n","test_scores = []\n","\n","# Create a list of different values for n_neighbors\n","neighbors = range(1, 21) # 1 to 20\n","\n","# Setup algorithm\n","knn = KNeighborsClassifier()\n","\n","# Loop through different neighbors values\n","for i in neighbors:\n","    knn.set_params(n_neighbors = i) # set neighbors value\n","\n","    # Fit the algorithm\n","    knn.fit(X_train, y_train)\n","\n","    # Update the training scores\n","    train_scores.append(knn.score(X_train, y_train))\n","\n","    # Update the test scores\n","    test_scores.append(knn.score(X_test, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["That was quick!\n","\n","Now let's look at KNN's train scores."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_scores"]},{"cell_type":"markdown","metadata":{},"source":["Ok, these are a bit hard to understand, so let's follow the data explorer's motto and *visualize, visualize, visualize!* In other words, let's plot them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(neighbors, train_scores, label=\"Train score\")\n","plt.plot(neighbors, test_scores, label=\"Test score\")\n","plt.xticks(np.arange(1, 21, 1))\n","plt.xlabel(\"Number of neighbors\")\n","plt.ylabel(\"Model score\")\n","plt.legend()\n","\n","print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["Looking at the graph, `n_neighbors = 11` seems best.\n","\n","Even knowing this, the `KNN`'s model performance didn't get near what `LogisticRegression` or the `RandomForestClassifier` did.\n","\n","Because of this, we'll discard `KNN` and focus on the other two.\n","\n","We've tuned `KNN` by hand but let's see how we can `LogisticsRegression` and `RandomForestClassifier` using [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n","\n","Instead of us having to manually try different hyperparameters by hand, `RandomizedSearchCV` tries a number of different combinations, evaluates them and saves the best."]},{"cell_type":"markdown","metadata":{},"source":["### Tuning models with with [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n","\n","Reading the Scikit-Learn documentation for [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV), we find there's a number of different hyperparameters we can tune.\n","\n","The same for [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n","\n","Let's create a hyperparameter grid (a dictionary of different hyperparameters) for each and then test them out.\n","\n","> **Note:** Be careful creating a hyperparameter dictionary for tuning as if there are typos in the keys of the dictionary, you will find that your code hyperparameter tuning code will produce errors."]},{"cell_type":"code","execution_count":191,"metadata":{},"outputs":[],"source":["# Different LogisticRegression hyperparameters\n","log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n","                \"solver\": [\"liblinear\"]}\n","\n","# Different RandomForestClassifier hyperparameters\n","rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n","           \"max_depth\": [None, 3, 5, 10],\n","           \"min_samples_split\": np.arange(2, 20, 2),\n","           \"min_samples_leaf\": np.arange(1, 20, 2)}"]},{"cell_type":"markdown","metadata":{},"source":["Now let's use [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to try and tune our `LogisticRegression` model.\n","\n","We'll pass it the different hyperparameters from `log_reg_grid` as well as set `n_iter=20`. This means, `RandomizedSearchCV` will try 20 different combinations of hyperparameters from `log_reg_grid` and save the best ones."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","# Setup random seed\n","np.random.seed(42)\n","\n","# Setup random hyperparameter search for LogisticRegression\n","rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n","                                param_distributions=log_reg_grid,\n","                                cv=5,\n","                                n_iter=20,\n","                                verbose=True)\n","\n","# Fit random hyperparameter search model\n","rs_log_reg.fit(X_train, y_train);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rs_log_reg.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rs_log_reg.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["Nice! That seems on par with the result we got before *without* any hyperparameter tuning.\n","\n","> **Note:** Many of the algorithms in Scikit-Learn have pretty good default hyperparameter values so don't be surprised if they perform pretty good on your data straight out of the box. But don't take this as being true all the time. Just because the default hyperparameters perform pretty well on your data doesn't mean there aren't a better set of hyperparameter values out there.\n","\n","Now we've tuned `LogisticRegression` using `RandomizedSearchCV`, we'll do the same for `RandomForestClassifier`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","# Setup random seed\n","np.random.seed(42)\n","\n","# Setup random hyperparameter search for RandomForestClassifier\n","rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n","                           param_distributions=rf_grid,\n","                           cv=5,\n","                           n_iter=20,\n","                           verbose=True)\n","\n","# Fit random hyperparameter search model\n","rs_rf.fit(X_train, y_train);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find the best parameters\n","rs_rf.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate the randomized search random forest model\n","rs_rf.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["Excellent! Tuning the hyperparameters for each model saw a slight performance boost in both the `RandomForestClassifier` and `LogisticRegression`.\n","\n","This is akin to tuning the settings on your oven and getting it to cook your favourite dish just right.\n","\n","But since `LogisticRegression` is pulling out in front, we'll try tuning it further with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."]},{"cell_type":"markdown","metadata":{},"source":["## Tuning a model with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n","\n","The difference between `RandomizedSearchCV` and `GridSearchCV` is:\n","\n","* [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) searches over a grid of hyperparameters performing `n_iter` combinations (e.g. will explore random combinations of the hyperparameters for a defined number of iterations).\n","* [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) will test every single possible combination of hyperparameters in the grid (this is a thorough test but can take quite a long time).\n","\n","Each class will save the best model at the end of testing.\n","\n","Let's see it in action."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","# Different LogisticRegression hyperparameters\n","log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n","                \"solver\": [\"liblinear\"]}\n","\n","# Setup grid hyperparameter search for LogisticRegression\n","gs_log_reg = GridSearchCV(LogisticRegression(),\n","                          param_grid=log_reg_grid,\n","                          cv=5,\n","                          verbose=True)\n","\n","# Fit grid hyperparameter search model\n","gs_log_reg.fit(X_train, y_train);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the best parameters\n","gs_log_reg.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate the model\n","gs_log_reg.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["In this case, we get the same results as before since our grid only has a maximum of 20 different hyperparameter combinations.\n","\n","> **Note:** If there are a large number of hyperparameter combinations in your grid, `GridSearchCV` may take a long time to try them all out. This is why it's a good idea to start with `RandomizedSearchCV`, try a certain amount of combinations and then use `GridSearchCV` to refine them."]},{"cell_type":"markdown","metadata":{},"source":["## Evaluating the classification model, beyond accuracy\n","\n","\n","| Metric/Evaluation Technique | Scikit-Learn method/documentation |\n","| ----- | ----- |\n","| ROC curve and AUC score | [`sklearn.metrics.RocCurveDisplay()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html), **Note:** This was previously `sklearn.metrics.plot_roc_curve()`, as of Scikit-Learn version 1.2+, it is `sklearn.metrics.RocCurveDisplay()`. |\n","| Confusion matrix | [`sklearn.metrics.confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) |\n","| Classification report | [`sklearn.metrics.classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) |\n","| Precision | [`sklearn.metrics.precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) |\n","| Recall | [`sklearn.metrics.recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) |\n","| F1-score | [`sklearn.metrics.f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) |"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[],"source":["# Make preidctions on test data\n","y_preds = gs_log_reg.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Let's see them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_preds"]},{"cell_type":"markdown","metadata":{},"source":["They look like our original test data labels, except different where the model has predicred wrong."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_test"]},{"cell_type":"markdown","metadata":{},"source":["Since we've got our prediction values we can find the metrics we want.\n","\n","Let's start with the ROC curve and AUC scores."]},{"cell_type":"markdown","metadata":{},"source":["## ROC Curve and AUC Scores\n","\n","Recall that the ROC curve is a way of understanding how your model is performing by comparing the true positive rate to the false positive rate.\n","\n","In our case...\n","\n","> To get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\n","\n","Scikit-Learn implements a function `RocCurveDisplay` (previously called `plot_roc_curve` in Scikit-Learn versions < 1.2) which can help us create a ROC curve as well as calculate the area under the curve (AUC) metric.\n","\n","Reading the documentation on the [`RocCurveDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html) function we can see it has a class method called [`from_estimator(estimator, X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_estimator) as inputs.\n","\n","Where `estimator` is a fitted machine learning model and `X` and `y` are the data you'd like to test it on.\n","\n","In our case, we'll use the GridSearchCV version of our `LogisticRegression` estimator, `gs_log_reg` as well as the test data, `X_test` and `y_test`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Before Scikit-Learn 1.2.0 (will error with versions 1.2+)\n","# from sklearn.metrics import plot_roc_curve\n","# plot_roc_curve(gs_log_reg, X_test, y_test);\n","\n","# Scikit-Learn 1.2.0 or later\n","from sklearn.metrics import RocCurveDisplay\n","\n","# from_estimator() = use a model to plot ROC curve on data\n","RocCurveDisplay.from_estimator(estimator=gs_log_reg,\n","                               X=X_test,\n","                               y=y_test);"]},{"cell_type":"markdown","metadata":{},"source":["This is great, our model does far better than guessing which would be a line going from the bottom left corner to the top right corner, AUC = 0.5.\n","\n","But a perfect model would achieve an AUC score of 1.0, so there's still room for improvement.\n","\n","Let's move on to the next evaluation request, a confusion matrix."]},{"cell_type":"markdown","metadata":{},"source":["## Creating a confusion matrix\n","\n","A confusion matrix is a visual way to show where your model made the right predictions and where it made the wrong predictions (or in other words, got confused).\n","\n","Scikit-Learn allows us to create a confusion matrix using [`sklearn.metrics.confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and passing it the true labels and predicted labels."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display confusion matrix\n","print(confusion_matrix(y_test, y_preds))"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, Scikit-Learn's built-in confusion matrix is a bit bland. For a presentation you'd probably want to make it visual.\n","\n","Let's create a function which uses Seaborn's [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for doing so."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Seaborn\n","import seaborn as sns\n","sns.set(font_scale=1.5) # Increase font size\n","\n","def plot_conf_mat(y_test, y_preds):\n","    \"\"\"\n","    Plots a confusion matrix using Seaborn's heatmap().\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize=(3, 3))\n","    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n","                     annot=True, # Annotate the boxes\n","                     cbar=False)\n","    plt.xlabel(\"true label\")\n","    plt.ylabel(\"predicted label\")\n","\n","plot_conf_mat(y_test, y_preds)"]},{"cell_type":"markdown","metadata":{},"source":["Beautiful! That looks much better.\n","\n","You can see the model gets confused (predicts the wrong label) relatively the same across both classes.\n","\n","In essence, there are 4 occasaions where the model predicted 0 when it should've been 1 (false negative) and 3 occasions where the model predicted 1 instead of 0 (false positive).\n","\n","As further evaluation, we could look into these samples and see why this may be the case."]},{"cell_type":"markdown","metadata":{},"source":["## Classification report\n","\n","A classification report is a collection of different metrics and other details.\n","\n","We can make a classification report using [`sklearn.metrics.classification_report(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and passing it the true labels as well as our models predicted labels.\n","\n","A classification report will also give us information on the precision and recall of our model for each class."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Show classification report\n","print(classification_report(y_test, y_preds))"]},{"cell_type":"markdown","metadata":{},"source":["What's going on here?\n","\n","Let's refresh ourselves on of the above metrics.\n","\n","| **Metric/metadata** | **Explanation** |\n","| ----- | ----- |\n","| **Precision** | Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0. |\n","| **Recall** | Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0. |\n","| **F1 score** | A combination of precision and recall. A perfect model achieves an F1 score of 1.0. |\n","| **Support** | The number of samples each metric was calculated on. |\n","| **Accuracy** | The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0. |\n","| **Macro avg** | Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesn’t class imbalance into effort, so if you do have class imbalances, pay attention to this metric. |\n","| **Weighted avg** | Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. will give a high value when one class out performs another due to having more samples). |\n","\n","Ok, now we've got a few deeper insights on our model.\n","\n","But these were all calculated using a single training and test set.\n","\n","What we'll do to make them more solid is calculate them using cross-validation.\n","\n","How?\n","\n","We'll take the best model along with the best hyperparameters and use [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) along with various `scoring` parameter values.\n","\n","`cross_val_score()` works by taking an estimator (machine learning model) along with data and labels.\n","\n","It then evaluates the machine learning model on the data and labels using cross-validation across `cv=5` (the default number of splits) splits and a defined `scoring` parameter.\n","\n","Let's remind ourselves of the best hyperparameters and then see them in action."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check best hyperparameters\n","gs_log_reg.best_params_"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[],"source":["# Import cross_val_score\n","from sklearn.model_selection import cross_val_score\n","\n","# Instantiate best model with best hyperparameters (found with GridSearchCV)\n","clf = LogisticRegression(C=0.23357214690901212,\n","                         solver=\"liblinear\")"]},{"cell_type":"markdown","metadata":{},"source":["Now we've got an instantiated classifier, let's find some cross-validated metrics."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","# Cross-validated accuracy score\n","cv_acc = cross_val_score(clf,\n","                         X,\n","                         y,\n","                         cv=5, # 5-fold cross-validation, this is the default\n","                         scoring=\"accuracy\") # accuracy as scoring\n","cv_acc"]},{"cell_type":"markdown","metadata":{},"source":["The output from `cross_val_score()` shows 5 different metrics across different splits of the data.\n","\n","This goes to show the power of cross-validation.\n","\n","If we had have only chosen to go with the results of one data split, we might be thinking our model is under performing or over performing.\n","\n","Since there are 5 metrics here, we'll take the average."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_acc = np.mean(cv_acc)\n","cv_acc"]},{"cell_type":"markdown","metadata":{},"source":["Now we'll do the same for other classification metrics."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cross-validated precision score\n","cv_precision = np.mean(cross_val_score(clf,\n","                                       X,\n","                                       y,\n","                                       cv=5, # 5-fold cross-validation\n","                                       scoring=\"precision\")) # precision as scoring\n","cv_precision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cross-validated recall score\n","cv_recall = np.mean(cross_val_score(clf,\n","                                    X,\n","                                    y,\n","                                    cv=5, # 5-fold cross-validation\n","                                    scoring=\"recall\")) # recall as scoring\n","cv_recall"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cross-validated F1 score\n","cv_f1 = np.mean(cross_val_score(clf,\n","                                X,\n","                                y,\n","                                cv=5, # 5-fold cross-validation\n","                                scoring=\"f1\")) # f1 as scoring\n","cv_f1"]},{"cell_type":"markdown","metadata":{},"source":["Okay, we've got cross validated metrics, now what?\n","\n","Let's visualize them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualizing cross-validated metrics\n","cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n","                            \"Precision\": cv_precision,\n","                            \"Recall\": cv_recall,\n","                            \"F1\": cv_f1},\n","                          index=[0])\n","cv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);"]},{"cell_type":"markdown","metadata":{},"source":["Great! This looks like something we could share. An extension might be adding the metrics on top of each bar so someone can quickly tell what they were.\n","\n","What now?\n","\n","The final thing to check off the list of our model evaluation techniques is feature importance."]},{"cell_type":"markdown","metadata":{},"source":["## Feature importance\n","\n","Feature importance is another way of asking, \"Which features contribute most to the outcomes of the model?\"\n","\n","For our problem, trying to predict heart disease using a patient's medical characteristics, getting the feature importance is like asking \"Which characteristics contribute most to a model predicting whether someone has heart disease or not?\"\n","\n","Because how each model finds patterns in data is slightly different, how a model judges how important those patterns are is different as well.\n","\n","This means for each model, there's a slightly different way of finding which features were most important and in turn, the feature importance of one model won't necessarily reflect the feature importance of another.\n","\n","You can usually find an example via the Scikit-Learn documentation or via searching for something like \"MODEL TYPE feature importance\", such as, \"random forest feature importance\".\n","\n","Since we're using `LogisticRegression`, we'll look at one way we can calculate feature importance for it.\n","\n","To do so, we'll use the `coef_` attribute. Looking at the [Scikit-Learn documentation for `LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), the `coef_` attribute is the coefficient of the features in the decision function.\n","\n","We can access the `coef_` attribute after we've fit an instance of `LogisticRegression`."]},{"cell_type":"code","execution_count":216,"metadata":{},"outputs":[],"source":["# Fit an instance of LogisticRegression (taken from above)\n","clf.fit(X_train, y_train);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check coef_\n","clf.coef_"]},{"cell_type":"markdown","metadata":{},"source":["Looking at this it might not make much sense. But these values are how much each feature contributes to how a model makes a decision on whether patterns in a sample of patients health data leans more towards having heart disease or not.\n","\n","Even knowing this, in it's current form, this `coef_` array still doesn't mean much. But it will if we combine it with the columns (features) of our dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Match features to columns\n","features_dict = dict(zip(df.columns, list(clf.coef_[0])))\n","features_dict"]},{"cell_type":"markdown","metadata":{},"source":["Now we've match the feature coefficients to different features, let's visualize them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize feature importance\n","features_df = pd.DataFrame(features_dict, index=[0])\n","features_df.T.plot.bar(title=\"Feature Importance\", legend=False);"]},{"cell_type":"markdown","metadata":{},"source":["You'll notice some are negative and some are positive.\n","\n","The larger the value (bigger bar), the more the feature contributes to the models decision.\n","\n","If the value is negative, it means there's a negative correlation. And vice versa for positive values.\n","\n","For example, the `sex` attribute has a negative value of -0.904, which means as the value for `sex` increases, the `target` value decreases.\n","\n","We can see this by comparing the `sex` column to the `target` column."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.crosstab(df[\"sex\"], df[\"target\"])"]},{"cell_type":"markdown","metadata":{},"source":["You can see, when `sex` is 0 (female), there are almost 3 times as many (72 vs. 24) people with heart disease (`target` = 1) than without.\n","\n","And then as `sex` increases to 1 (male), the ratio goes down to almost 1 to 1 (114 vs. 93) of people who have heart disease and who don't.\n","\n","What does this mean?\n","\n","It means the model has found a pattern which reflects the data. Looking at these figures and this specific dataset, it seems if the patient is female, they're more likely to have heart disease.\n","\n","How about a positive correlation?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Contrast slope (positive coefficient) with target\n","pd.crosstab(df[\"slope\"], df[\"target\"])"]},{"cell_type":"markdown","metadata":{},"source":["Looking back the data dictionary, we see `slope` is the \"slope of the peak exercise ST segment\" where:\n","* 0: Upsloping: better heart rate with excercise (uncommon)\n","* 1: Flatsloping: minimal change (typical healthy heart)\n","* 2: Downslopins: signs of unhealthy heart\n","    \n","According to the model, there's a positive correlation of 0.470, not as strong as `sex` and `target` but still more than 0.\n","\n","This positive correlation means our model is picking up the pattern that as `slope` increases, so does the `target` value.\n","\n","Is this true?\n","\n","When you look at the contrast (`pd.crosstab(df[\"slope\"], df[\"target\"]`) it is. As `slope` goes up, so does `target`.\n","\n","What can you do with this information?\n","\n","This is something you might want to talk to a subject matter expert about.\n","\n","They may be interested in seeing where machine learning model is finding the most patterns (highest correlation) as well as where it's not (lowest correlation).\n","\n","Doing this has a few benefits:\n","1. **Finding out more** - If some of the correlations and feature importances are confusing, a subject matter expert may be able to shed some light on the situation and help you figure out more.\n","2. **Redirecting efforts** - If some features offer far more value than others, this may change how you collect data for different problems. See point 3.\n","3. **Less but better** - Similar to above, if some features are offering far more value than others, you could reduce the number of features your model tries to find patterns in as well as improve the ones which offer the most. This could potentially lead to saving on computation, by having a model find patterns across less features, whilst still achieving the same performance levels."]},{"cell_type":"markdown","metadata":{},"source":["## Experimentation\n","\n","We've completed all the metrics!\n","\n","You should be able to put together a great report containing a confusion matrix, and a handful of cross-validated metrics such as precision, recall and F1-score and you can even include which features contribute most to the model making a decision.\n","\n","But after all this you might be wondering where step 6 in the framework is, experimentation.\n","\n","Well the secret here is, as you might've guessed, the whole thing is experimentation.\n","\n","From trying different models, to tuning different models to figuring out which hyperparameters were best.\n","\n","What we've worked through so far has been a series of experiments.\n","\n","And the truth is, we could keep going. But of course, things can't go on forever.\n","\n","So by this stage, after trying a few different things, we'd ask ourselves did we meet the evaluation metric?\n","\n","Remember we defined one in step 3.\n","\n","> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue this project.\n","\n","In this case, we didn't.\n","\n","The highest accuracy our model achieved was below 90%."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":654897,"sourceId":1157702,"sourceType":"datasetVersion"}],"dockerImageVersionId":30357,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
